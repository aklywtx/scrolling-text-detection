{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 56,
   "metadata": {},
   "outputs": [],
   "source": [
    "from PIL import Image\n",
    "import os, sys\n",
    "import cv2\n",
    "import numpy as np\n",
    "from imutils.object_detection import non_max_suppression\n",
    "import random"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 57,
   "metadata": {},
   "outputs": [],
   "source": [
    "random.seed(42)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 65,
   "metadata": {},
   "outputs": [],
   "source": [
    "# some hyperparameter\n",
    "frame_interval = 25\n",
    "east_confidence = 0.5\n",
    "opflow_min_area_threshold = 700\n",
    "opflow_height_increase = 0\n",
    "num_frames_for_overlap_check = 3\n",
    "rolling_text_confidence = 0.2 # if the overlapping area / east box area > 0.2, this is a part of the rolling text"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [],
   "source": [
    "# file path\n",
    "vidpath = '../data/static_moving.mp4'"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Extract frames"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 46,
   "metadata": {},
   "outputs": [],
   "source": [
    "def files(vidpath):\n",
    "    base_name = os.path.basename(vidpath)\n",
    "    vidname, _ = os.path.splitext(base_name)  # Split the base name to get the name without the extension\n",
    "    \n",
    "    try: # Remove image frames path if it already exists\n",
    "        os.remove(f\"{vidname}_image_frames\")\n",
    "    except OSError:\n",
    "        pass\n",
    "    if not os.path.exists(f\"{vidname}_image_frames\"): # create the directory if it does not already exist\n",
    "        os.makedirs(f\"{vidname}_image_frames\")\n",
    "\n",
    "    src_vid = cv2.VideoCapture(vidpath) # open the video file\n",
    "    return(src_vid)\n",
    "    \n",
    "def process(src_vid, frame_interval):\n",
    "    base_name = os.path.basename(vidpath)\n",
    "    vidname, _ = os.path.splitext(base_name)  # Split the base name to get the name without the extension\n",
    "    saved_frame_nums = []\n",
    "    \n",
    "    index = 0\n",
    "    while src_vid.isOpened():\n",
    "        ret, frame = src_vid.read() \n",
    "        if not ret: # break at the end of the video (ret returns True if frame is succesfully read)\n",
    "            break\n",
    "        name = f'./{vidname}_image_frames/frame{str(index)}.png'\n",
    "\n",
    "        if index % frame_interval == 0: # every 50th frame will be saved: can adjust this number to capture more or less frames\n",
    "            print('Extracting frame...' + name)\n",
    "            cv2.imwrite(name, frame)\n",
    "            saved_frame_nums.append(index)\n",
    "        index = index + 1\n",
    "        \n",
    "    src_vid.release()\n",
    "    cv2.destroyAllWindows()\n",
    "    \n",
    "    video_frames_dir = f\"./{vidname}_image_frames\"\n",
    "    \n",
    "    return saved_frame_nums, video_frames_dir"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 47,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Extracting frame..../static_moving_image_frames/frame0.png\n",
      "Extracting frame..../static_moving_image_frames/frame25.png\n",
      "Extracting frame..../static_moving_image_frames/frame50.png\n",
      "Extracting frame..../static_moving_image_frames/frame75.png\n",
      "Extracting frame..../static_moving_image_frames/frame100.png\n",
      "Extracting frame..../static_moving_image_frames/frame125.png\n",
      "Extracting frame..../static_moving_image_frames/frame150.png\n",
      "Extracting frame..../static_moving_image_frames/frame175.png\n",
      "Extracting frame..../static_moving_image_frames/frame200.png\n",
      "Extracting frame..../static_moving_image_frames/frame225.png\n",
      "Extracting frame..../static_moving_image_frames/frame250.png\n",
      "Extracting frame..../static_moving_image_frames/frame275.png\n",
      "Extracting frame..../static_moving_image_frames/frame300.png\n"
     ]
    }
   ],
   "source": [
    "vid = files(vidpath)\n",
    "saved_frame_nums, video_frames_dir = process(vid, frame_interval)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Optical Flow"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 28,
   "metadata": {},
   "outputs": [],
   "source": [
    "def draw_horizontal_flow(img, flow, step=30):\n",
    "    \"\"\"Draw optical flow vectors on the video.\"\"\"\n",
    "    h, w = img.shape[:2]\n",
    "    y, x = np.mgrid[step//2:h:step, step//2:w:step].reshape(2, -1).astype(int)\n",
    "    fx = flow[y, x, 0]\n",
    "    fy = flow[y, x, 1]\n",
    "\n",
    "    # Keep only leftward motion vectors (negative horizontal flow)\n",
    "    fx[fx > 0] = 0\n",
    "\n",
    "    lines = np.vstack([x, y, x + fx, y + fy]).T.reshape(-1, 2, 2)\n",
    "    lines = np.int32(lines + 0.5)\n",
    "\n",
    "    vis = cv2.cvtColor(img, cv2.COLOR_GRAY2BGR)\n",
    "    for (x1, y1), (x2, y2) in lines:\n",
    "        cv2.arrowedLine(vis, (x1, y1), (x2, y2), (0, 255, 0), 1, tipLength=0.5)\n",
    "    return vis\n",
    "\n",
    "def detect_and_save_screengrab(video_path):\n",
    "    cap = cv2.VideoCapture(video_path)\n",
    "    \n",
    "    # Get total number of frames and calculate halfway point\n",
    "    total_frames = int(cap.get(cv2.CAP_PROP_FRAME_COUNT))\n",
    "    halfway_frame = total_frames // 2\n",
    "\n",
    "    ret, first_frame = cap.read()\n",
    "    if not ret:\n",
    "        print(\"Failed to read the video file.\")\n",
    "        return\n",
    "\n",
    "    prev_gray = cv2.cvtColor(first_frame, cv2.COLOR_BGR2GRAY)\n",
    "\n",
    "    # Initialize a mask for consistent leftward movement\n",
    "    consistent_motion_mask = None\n",
    "    consistency_threshold = 0.8  # Consistent movement threshold (percentage of frames)\n",
    "    num_frames = 0\n",
    "\n",
    "    saved_screengrab = False\n",
    "    screengrab_frame = None\n",
    "\n",
    "    while cap.isOpened():\n",
    "        ret, frame = cap.read()\n",
    "        if not ret:\n",
    "            break\n",
    "        \n",
    "        gray = cv2.cvtColor(frame, cv2.COLOR_BGR2GRAY)\n",
    "        \n",
    "        flow = cv2.calcOpticalFlowFarneback(prev_gray, gray, None, 0.5, 5, 15, 3, 5, 1.2, 0)\n",
    "        \n",
    "        # Calculate the mask of leftward movement\n",
    "        leftward_motion_mask = flow[..., 0] < 0\n",
    "        \n",
    "        # Initialize the consistent motion mask\n",
    "        if consistent_motion_mask is None:\n",
    "            consistent_motion_mask = np.zeros_like(leftward_motion_mask, dtype=np.float32)\n",
    "        \n",
    "        # Update the consistent leftward movement mask\n",
    "        consistent_motion_mask[leftward_motion_mask] += 1\n",
    "        num_frames += 1\n",
    "\n",
    "        # Calculate the ratio of consistent leftward movement\n",
    "        consistent_ratio = consistent_motion_mask / num_frames\n",
    "        consistent_leftward_regions = consistent_ratio > consistency_threshold\n",
    "\n",
    "        # Apply morphological operations to fill in the gaps\n",
    "        consistent_leftward_mask = consistent_leftward_regions.astype(np.uint8)\n",
    "        kernel = np.ones((5, 5), np.uint8)\n",
    "        consistent_leftward_mask = cv2.morphologyEx(consistent_leftward_mask, cv2.MORPH_CLOSE, kernel)\n",
    "\n",
    "        # Create an image for highlighting\n",
    "        consistent_leftward_img = np.zeros_like(frame)\n",
    "        consistent_leftward_img[consistent_leftward_mask == 1] = [0, 255, 0]  # Green\n",
    "\n",
    "        vis = draw_horizontal_flow(gray, flow)\n",
    "\n",
    "        # Combine visualization with consistent leftward movement highlight\n",
    "        combined_vis = cv2.addWeighted(vis, 0.7, consistent_leftward_img, 0.3, 0)\n",
    "\n",
    "        # Save screengrab at halfway point\n",
    "        if num_frames == halfway_frame and not saved_screengrab:\n",
    "            screengrab_frame = combined_vis.copy()\n",
    "            saved_screengrab = True\n",
    "        \n",
    "        prev_gray = gray.copy()\n",
    "\n",
    "    # Save the screengrab outside the loop to avoid delay\n",
    "    if screengrab_frame is not None:\n",
    "        vidname = str(video_path)\n",
    "        cv2.imwrite(f'{vidname}_screengrab_halfway.png', screengrab_frame)\n",
    "    \n",
    "    cap.release()\n",
    "    cv2.destroyAllWindows()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 29,
   "metadata": {},
   "outputs": [],
   "source": [
    "########## CALL TO FUNCTION:\n",
    "detect_and_save_screengrab(vidpath)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Draw a bounding box around the detected regions"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 53,
   "metadata": {},
   "outputs": [],
   "source": [
    "def extract_bounding_boxes(vidpath, height_increase=0, opflow_min_area_threshold=700):\n",
    "    opflow_box_coordinates = []\n",
    "    screengrab_path = f'{vidpath}_screengrab_halfway.png'\n",
    "    \n",
    "    screengrab = cv2.imread(screengrab_path)\n",
    "    if screengrab is None:\n",
    "        print(\"Failed to read the screengrab.\")\n",
    "        return\n",
    "\n",
    "    # Convert to HSV and create a mask for the green color\n",
    "    hsv = cv2.cvtColor(screengrab, cv2.COLOR_BGR2HSV)\n",
    "    lower_green = np.array([40, 40, 40])\n",
    "    upper_green = np.array([80, 255, 255])\n",
    "    mask = cv2.inRange(hsv, lower_green, upper_green)\n",
    "\n",
    "    # Find contours\n",
    "    contours, _ = cv2.findContours(mask, cv2.RETR_EXTERNAL, cv2.CHAIN_APPROX_SIMPLE)\n",
    "\n",
    "    # Draw bounding boxes around significant motion regions\n",
    "    for contour in contours:\n",
    "        if cv2.contourArea(contour) > opflow_min_area_threshold:  # Adjust the threshold for minimum area as needed\n",
    "            x, y, w, h = cv2.boundingRect(contour)\n",
    "            # Increase the height of the bounding box\n",
    "            y = max(0, y - height_increase // 2)\n",
    "            h = h + height_increase\n",
    "            # Ensure the bounding box is within the image boundaries\n",
    "            h = min(h, screengrab.shape[0] - y)\n",
    "            cv2.rectangle(screengrab, (x, y), (x + w, y + h), (0, 255, 0), 2)\n",
    "            opflow_box_coordinates += [[x, y, x+w, y+h]]\n",
    "\n",
    "    # Save and display the image with bounding boxes\n",
    "    cv2.imwrite(f'{vidpath}_screengrab_with_bounding_boxes.png', screengrab)\n",
    "    \n",
    "    return opflow_box_coordinates\n",
    "\n",
    "########## CALL TO FUNCTION:\n",
    "opflow_box_coordinates = extract_bounding_boxes(vidpath, opflow_height_increase, opflow_min_area_threshold) # Add extra height to ensure no text is cut off from the top/bottom"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### EAST for text detection"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 52,
   "metadata": {},
   "outputs": [],
   "source": [
    "def east_detect(image, output_path, east_confidence):\n",
    "    ''' uses EAST detection to get x,y coordinates of all bounding boxes of input image \n",
    "    \n",
    "    param images: paths to the image\n",
    "    param output_path: path where to save copy of original image with bounding boxes drawn on top\n",
    "    \n",
    "    returns list of tuples: each tuple contains (x_min, y_min, x_max, y_max) of each detected bounding box\n",
    "\n",
    "    code from: https://medium.com/technovators/scene-text-detection-in-python-with-east-and-craft-cbe03dda35d5\n",
    "    '''\n",
    "    \n",
    "    \n",
    "    layerNames = [\n",
    "    \t\"feature_fusion/Conv_7/Sigmoid\",\n",
    "    \t\"feature_fusion/concat_3\"]\n",
    "    \n",
    "    orig = image.copy()\n",
    "    \n",
    "    if len(image.shape) == 2:\n",
    "        image = cv2.cvtColor(image, cv2.COLOR_GRAY2RGB)\n",
    "    \n",
    "    (H, W) = image.shape[:2]\n",
    "    \n",
    "    # set the new width and height and then determine the ratio in change\n",
    "    # for both the width and height: Should be multiple of 32\n",
    "    (newW, newH) = (672, 672)\n",
    "    \n",
    "    rW = W / float(newW)\n",
    "    rH = H / float(newH)\n",
    "    \n",
    "    # resize the image and grab the new image dimensions\n",
    "    image = cv2.resize(image, (newW, newH))\n",
    "    \n",
    "    (H, W) = image.shape[:2]\n",
    "    \n",
    "    net = cv2.dnn.readNet(\"frozen_east_text_detection.pb\")\n",
    "    \n",
    "    blob = cv2.dnn.blobFromImage(image, 1.0, (W, H),\n",
    "    \t(123.68, 116.78, 103.94), swapRB=True, crop=False)\n",
    "    \n",
    "    net.setInput(blob)\n",
    "    \n",
    "    (scores, geometry) = net.forward(layerNames)\n",
    "    \n",
    "    (numRows, numCols) = scores.shape[2:4]\n",
    "    rects = []\n",
    "    confidences = []\n",
    "    # loop over the number of rows\n",
    "    for y in range(0, numRows):\n",
    "        # extract the scores (probabilities), followed by the geometrical\n",
    "        # data used to derive potential bounding box coordinates that\n",
    "        # surround text\n",
    "        scoresData = scores[0, 0, y]\n",
    "        xData0 = geometry[0, 0, y]\n",
    "        xData1 = geometry[0, 1, y]\n",
    "        xData2 = geometry[0, 2, y]\n",
    "        xData3 = geometry[0, 3, y]\n",
    "        anglesData = geometry[0, 4, y]\n",
    "    \n",
    "        for x in range(0, numCols):\n",
    "    \t\t# if our score does not have sufficient probability, ignore it\n",
    "            # Set minimum confidence as required\n",
    "            if scoresData[x] < east_confidence:\n",
    "                continue\n",
    "    \t\t# compute the offset factor as our resulting feature maps will\n",
    "            #  x smaller than the input image\n",
    "            (offsetX, offsetY) = (x * 4.0, y * 4.0)\n",
    "            # extract the rotation angle for the prediction and then\n",
    "            # compute the sin and cosine\n",
    "            angle = anglesData[x]\n",
    "            cos = np.cos(angle)\n",
    "            sin = np.sin(angle)\n",
    "            # use the geometry volume to derive the width and height of\n",
    "            # the bounding box\n",
    "            h = xData0[x] + xData2[x]\n",
    "            w = xData1[x] + xData3[x]\n",
    "            # compute both the starting and ending (x, y)-coordinates for\n",
    "            # the text prediction bounding box\n",
    "            endX = int(offsetX + (cos * xData1[x]) + (sin * xData2[x]))\n",
    "            endY = int(offsetY - (sin * xData1[x]) + (cos * xData2[x]))\n",
    "            startX = int(endX - w)\n",
    "            startY = int(endY - h)\n",
    "            # add the bounding box coordinates and probability score to\n",
    "            # our respective lists\n",
    "            rects.append((startX, startY, endX, endY))\n",
    "            confidences.append(scoresData[x])\n",
    "                        \n",
    "    boxes = non_max_suppression(np.array(rects), probs=confidences)\n",
    "    # loop over the bounding boxes\n",
    "    coordinates = []\n",
    "    for (startX, startY, endX, endY) in boxes:\n",
    "        # scale the bounding box coordinates based on the respective\n",
    "        # ratios\n",
    "        startX = int(startX * rW)\n",
    "        startY = int(startY * rH)\n",
    "        endX = int(endX * rW)\n",
    "        endY = int(endY * rH)\n",
    "        # draw the bounding box on the image\n",
    "        # startX,startY ------\n",
    "        # |                 |\n",
    "        # |                 |\n",
    "        # |                 |\n",
    "        # ----------endX,endY\n",
    "        out_image = cv2.rectangle(orig, (startX, startY), (endX, endY), (0, 255, 0), 2)\n",
    "        # out_image = cv2.rectangle(out_image, (146, 888), (1920, 1002), (0, 255, 0), 2)\n",
    "        coordinates += [[startX, startY, endX, endY]]\n",
    "    \n",
    "    cv2.imwrite(f\"{output_path}\", out_image)\n",
    "    return coordinates\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 51,
   "metadata": {},
   "outputs": [],
   "source": [
    "coordinates_list= []\n",
    "\n",
    "for frame_number in saved_frame_nums:\n",
    "\tframe_path = video_frames_dir + f\"/frame{frame_number}.png\" # path to individual frame being processed\n",
    "\n",
    "\tbase_name = os.path.basename(frame_path)\n",
    "\tframe_name, _ = os.path.splitext(base_name) # get just the individual frame name without the extension\n",
    "\n",
    "\tif not os.path.isdir(f\"{video_frames_dir}/output\"):\n",
    "\t\tos.makedirs(f\"{video_frames_dir}/output\") # makes output directory for the frame being processed\n",
    "\t\n",
    "\timage = cv2.imread(frame_path) \n",
    "\tcoordinates = east_detect(image, f\"{video_frames_dir}/output/out_{frame_name}_EAST.png\", east_confidence) # get coordinates of bounding boxes and save image to new output dir for that frame\n",
    "\n",
    "\tcoordinates_list += [coordinates]\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Calculate the overlapping area of optic flow rectangles and east rectangles.\n",
    "\n",
    "randomly get {N} frames to do this:\n",
    "* For each opflow rectangle, get the overlapping area sum \n",
    "* The rectangle with the largest overlapping area -> rolling text region \\\n",
    "-> some sanity check for rectangles?"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 54,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[[87, 896, 136, 1000], [146, 888, 1920, 1002]]"
      ]
     },
     "execution_count": 54,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "opflow_box_coordinates"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 50,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[[[1065, 506, 1220, 570],\n",
       "  [1548, 906, 1862, 975],\n",
       "  [705, 503, 1077, 570],\n",
       "  [1062, 901, 1248, 980],\n",
       "  [1274, 901, 1540, 983]],\n",
       " [[1065, 506, 1220, 570],\n",
       "  [1197, 908, 1520, 977],\n",
       "  [728, 904, 914, 977],\n",
       "  [702, 503, 1074, 570],\n",
       "  [1531, 906, 1702, 981],\n",
       "  [1734, 912, 1917, 981],\n",
       "  [937, 896, 1214, 978]],\n",
       " [[1762, 917, 1931, 975],\n",
       "  [1062, 504, 1220, 570],\n",
       "  [402, 903, 577, 975],\n",
       "  [877, 904, 1191, 973],\n",
       "  [702, 503, 1077, 570],\n",
       "  [1194, 904, 1371, 983],\n",
       "  [1368, 903, 1751, 990],\n",
       "  [597, 896, 874, 985],\n",
       "  [1051, 626, 1105, 657]],\n",
       " [[1065, 504, 1222, 570],\n",
       "  [1425, 919, 1648, 975],\n",
       "  [868, 906, 1034, 980],\n",
       "  [1657, 901, 1822, 977],\n",
       "  [705, 503, 1077, 570],\n",
       "  [551, 906, 851, 975],\n",
       "  [262, 901, 534, 983],\n",
       "  [60, 903, 242, 978],\n",
       "  [1034, 903, 1411, 991],\n",
       "  [1054, 626, 1105, 657]],\n",
       " [[1085, 914, 1300, 973],\n",
       "  [1062, 506, 1222, 572],\n",
       "  [1717, 904, 1908, 985],\n",
       "  [202, 911, 514, 978],\n",
       "  [1322, 904, 1485, 975],\n",
       "  [705, 503, 1077, 570],\n",
       "  [1494, 909, 1697, 985],\n",
       "  [0, 903, 182, 975],\n",
       "  [522, 904, 700, 983],\n",
       "  [1048, 623, 1108, 658],\n",
       "  [688, 896, 1085, 985]],\n",
       " [[745, 914, 968, 977],\n",
       "  [1062, 503, 1225, 570],\n",
       "  [-2, 914, 174, 975],\n",
       "  [197, 911, 360, 978],\n",
       "  [1602, 901, 1882, 977],\n",
       "  [1377, 901, 1580, 988],\n",
       "  [705, 503, 1077, 570],\n",
       "  [1048, 625, 1111, 655],\n",
       "  [985, 901, 1142, 980],\n",
       "  [1162, 912, 1368, 988],\n",
       "  [360, 898, 748, 991]],\n",
       " [[1062, 506, 1220, 572],\n",
       "  [420, 916, 622, 975],\n",
       "  [1268, 901, 1528, 973],\n",
       "  [1548, 900, 1825, 975],\n",
       "  [702, 503, 1077, 570],\n",
       "  [651, 900, 811, 975],\n",
       "  [817, 909, 1025, 986],\n",
       "  [1040, 900, 1240, 988],\n",
       "  [1048, 625, 1108, 657],\n",
       "  [34, 900, 405, 986],\n",
       "  [1188, 284, 1305, 342]],\n",
       " [[1065, 506, 1220, 572],\n",
       "  [80, 914, 294, 975],\n",
       "  [928, 898, 1205, 975],\n",
       "  [320, 903, 471, 975],\n",
       "  [1202, 901, 1491, 975],\n",
       "  [702, 503, 1077, 570],\n",
       "  [700, 904, 905, 986],\n",
       "  [497, 908, 694, 988],\n",
       "  [1048, 626, 1108, 657],\n",
       "  [1208, 278, 1325, 339]],\n",
       " [[1062, 506, 1222, 570],\n",
       "  [891, 898, 1145, 975],\n",
       "  [705, 503, 1077, 570],\n",
       "  [171, 919, 360, 986],\n",
       "  [602, 900, 882, 973],\n",
       "  [377, 901, 580, 990],\n",
       "  [1048, 626, 1108, 657],\n",
       "  [-14, 908, 128, 977]],\n",
       " [[1065, 504, 1220, 570],\n",
       "  [262, 901, 522, 973],\n",
       "  [705, 503, 1077, 570],\n",
       "  [545, 898, 828, 973],\n",
       "  [37, 903, 234, 988],\n",
       "  [1048, 626, 1111, 657],\n",
       "  [1368, 927, 1537, 953]],\n",
       " [[1065, 501, 1228, 568],\n",
       "  [214, 901, 477, 975],\n",
       "  [702, 503, 1077, 570],\n",
       "  [2, 900, 177, 975],\n",
       "  [1048, 621, 1108, 657]],\n",
       " [[1065, 503, 1225, 568], [702, 503, 1077, 570], [1048, 626, 1100, 660]],\n",
       " [[1062, 507, 1217, 570], [702, 506, 1074, 573]]]"
      ]
     },
     "execution_count": 50,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "coordinates_list"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 59,
   "metadata": {},
   "outputs": [],
   "source": [
    "def calculate_overlap_area(rect1, rect2):\n",
    "    \"\"\"\n",
    "    Calculate the overlapping area between two rectangles.\n",
    "    \n",
    "    rect1 and rect2 are tuples in the form (startX, startY, endX, endY)\n",
    "    \"\"\"\n",
    "    x1_max = max(rect1[0], rect2[0])\n",
    "    y1_max = max(rect1[1], rect2[1])\n",
    "    x2_min = min(rect1[2], rect2[2])\n",
    "    y2_min = min(rect1[3], rect2[3])\n",
    "    \n",
    "    overlap_width = max(0, x2_min - x1_max)\n",
    "    overlap_height = max(0, y2_min - y1_max)\n",
    "    \n",
    "    return overlap_width * overlap_height\n",
    "\n",
    "\n",
    "def sum_overlapping_areas(main_rect, rectangles):\n",
    "    \"\"\"\n",
    "    Calculate the sum of overlapping areas of multiple rectangles with the main rectangle.\n",
    "    \n",
    "    main_rect is a tuple in the form (startX, startY, endX, endY)\n",
    "    rectangles is a list of tuples in the same form\n",
    "    \"\"\"\n",
    "    total_overlap_area = 0\n",
    "    for rect in rectangles:\n",
    "        total_overlap_area += calculate_overlap_area(main_rect, rect)\n",
    "    \n",
    "    return total_overlap_area\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 62,
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "overlap_areas = []\n",
    "choose_frames = random.sample(list(range(len(saved_frame_nums))), num_frames_for_overlap_check)\n",
    "\n",
    "for opflow_box_coordinate in opflow_box_coordinates:\n",
    "    total_overlap = 0\n",
    "    for frame_index in choose_frames:\n",
    "        total_overlap += sum_overlapping_areas(opflow_box_coordinate, coordinates_list[frame_index])\n",
    "    overlap_areas.append(total_overlap)\n",
    "\n",
    "rolling_box_index = np.argmax(np.array(overlap_areas))\n",
    "rolling_box = opflow_box_coordinates[rolling_box_index]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### After getting the rolling box\n",
    "\n",
    "* For all the east boxes in each frame, calculate their overlapping area with the rolling box\n",
    "* If overlapping area / east box area >= rolling_text_confidence, this is a part of the rolling text\n",
    "* Crop this part and save to the folder in each frame. \\\n",
    "-> crop each word from left to right?"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "base",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.13"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
