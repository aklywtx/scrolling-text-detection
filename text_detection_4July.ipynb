{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "a251769f-5c80-4873-8e59-7bad1b94a830",
   "metadata": {},
   "source": [
    "### read me\n",
    "so far this code will:  \n",
    "1. extract video frames from the given video (you can adjust the number of frames it creates in code cell 2)  \n",
    "   FYI: a new dir will be created called vidname_image_frames which will store all of the extracted image frames from the vid\n",
    "3. use EAST to extract bounding boxes over one given frame (**it only processes one frame rn**)  \n",
    "   --  a new dir. will be created called out_framename, which will contain the crops of the individual words determined by the bounding boxes and also the original image with the green bounding boxes drawn over it   \n",
    "   -- I used some code I found online for this and tweaked it a bit  \n",
    "   -- If you want to get different bounding boxes, you can adjust this line in cell 3: (newW, newH) = (672, 672) to different numbers and it'll give you different bounding boxes. tbh I don't really know what this means or how to logically choose these numbers: they just need to be multiples of 32 or it'll throw an error. These values gave me the best results for this frame lol\n",
    "\n",
    "**You need to update the following paths to test on your own machine:**\n",
    "1. cell 2: vidpath (the path to the video you want to process: i'm using testseq.mp4 from the google drive)  \n",
    "2. cell 3: video_frames_dir (the path to the dir. created in step 1, which holds the extracted frames)  \n",
    "3. cell 3: frame_path (the path to the specific frame that you want to process)\n",
    "\n",
    "**You also need to save the pre-trained model weights file to your root dir/wherever ur running this notebook**  \n",
    "frozen_east_text_detection.pb\n",
    "\n",
    "in the end your rootdir will contain:  \n",
    "\n",
    "thisNotebook.ipynb  \n",
    "frozen_east_text_detection.pb  \n",
    "testvid.mp4  \n",
    "testseq_image_frames  \n",
    "----> frames 1-n  \n",
    "out_framename   \n",
    "----> crops of individual words  \n",
    "----> original frame with bounding boxes drawn on top "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "05a24255-7908-4d69-a0d1-4e71f0cb3cb7",
   "metadata": {},
   "outputs": [],
   "source": [
    "from PIL import Image\n",
    "import os, sys\n",
    "import cv2\n",
    "import numpy as np\n",
    "from imutils.object_detection import non_max_suppression"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "1383ba83-a566-4e90-be35-e25bf30536f7",
   "metadata": {},
   "source": [
    "### Extract Video Frames from Video"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "538f6e47-55d2-41a2-b404-3c49723d8f34",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Extracting frame..../testseq_image_frames/frame0.png\n",
      "Extracting frame..../testseq_image_frames/frame50.png\n",
      "Extracting frame..../testseq_image_frames/frame100.png\n",
      "Extracting frame..../testseq_image_frames/frame150.png\n",
      "Extracting frame..../testseq_image_frames/frame200.png\n",
      "Extracting frame..../testseq_image_frames/frame250.png\n",
      "Extracting frame..../testseq_image_frames/frame300.png\n"
     ]
    }
   ],
   "source": [
    "def files(vidpath):\n",
    "    base_name = os.path.basename(vidpath)\n",
    "    vidname, _ = os.path.splitext(base_name)  # Split the base name to get the name without the extension\n",
    "    \n",
    "    try: # Remove image frames path if it already exists\n",
    "        os.remove(f\"{vidname}_image_frames\")\n",
    "    except OSError:\n",
    "        pass\n",
    "    if not os.path.exists(f\"{vidname}_image_frames\"): # create the directory if it does not already exist\n",
    "        os.makedirs(f\"{vidname}_image_frames\")\n",
    "\n",
    "    src_vid = cv2.VideoCapture(vidpath) # open the video file\n",
    "    return(src_vid)\n",
    "    \n",
    "def process(src_vid):\n",
    "    base_name = os.path.basename(vidpath)\n",
    "    vidname, _ = os.path.splitext(base_name)  # Split the base name to get the name without the extension\n",
    "    \n",
    "    index = 0\n",
    "    while src_vid.isOpened():\n",
    "        ret, frame = src_vid.read() \n",
    "        if not ret: # break at the end of the video (ret returns True if frame is succesfully read)\n",
    "            break\n",
    "        name = f'./{vidname}_image_frames/frame{str(index)}.png'\n",
    "\n",
    "        if index % 50 == 0: # every 50th frame will be saved: can adjust this number to capture more or less frames\n",
    "            print('Extracting frame...' + name)\n",
    "            cv2.imwrite(name, frame)\n",
    "        index = index + 1\n",
    "        \n",
    "    src_vid.release()\n",
    "    cv2.destroyAllWindows()\n",
    "\n",
    "if __name__ == '__main__':\n",
    "    vidpath = 'testseq.mp4'\n",
    "    vid = files(vidpath)\n",
    "    process(vid)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "61862b46-0616-470c-bee7-c15a46afbadc",
   "metadata": {},
   "source": [
    "### EAST for text detection"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "ba893185-e1fb-4834-95df-0675f3eb30a5",
   "metadata": {},
   "outputs": [],
   "source": [
    "def east_detect(image, output_path):\n",
    "    ''' uses EAST detection to get x,y coordinates of all bounding boxes of input image \n",
    "    \n",
    "    param image: path to image\n",
    "    param output_path: path where to save copy of original image with bounding boxes drawn on top\n",
    "    \n",
    "    returns list of tuples: each tuple contains (x_min, y_min, x_max, y_max) of each detected bounding box\n",
    "\n",
    "    code from: https://medium.com/technovators/scene-text-detection-in-python-with-east-and-craft-cbe03dda35d5\n",
    "    '''\n",
    "    \n",
    "    layerNames = [\n",
    "    \t\"feature_fusion/Conv_7/Sigmoid\",\n",
    "    \t\"feature_fusion/concat_3\"]\n",
    "    \n",
    "    orig = image.copy()\n",
    "    \n",
    "    if len(image.shape) == 2:\n",
    "        image = cv2.cvtColor(image, cv2.COLOR_GRAY2RGB)\n",
    "    \n",
    "    (H, W) = image.shape[:2]\n",
    "    \n",
    "    # set the new width and height and then determine the ratio in change\n",
    "    # for both the width and height: Should be multiple of 32\n",
    "    (newW, newH) = (672, 672)\n",
    "    \n",
    "    rW = W / float(newW)\n",
    "    rH = H / float(newH)\n",
    "    \n",
    "    # resize the image and grab the new image dimensions\n",
    "    image = cv2.resize(image, (newW, newH))\n",
    "    \n",
    "    (H, W) = image.shape[:2]\n",
    "    \n",
    "    net = cv2.dnn.readNet(\"frozen_east_text_detection.pb\")\n",
    "    \n",
    "    blob = cv2.dnn.blobFromImage(image, 1.0, (W, H),\n",
    "    \t(123.68, 116.78, 103.94), swapRB=True, crop=False)\n",
    "    \n",
    "    net.setInput(blob)\n",
    "    \n",
    "    (scores, geometry) = net.forward(layerNames)\n",
    "    \n",
    "    (numRows, numCols) = scores.shape[2:4]\n",
    "    rects = []\n",
    "    confidences = []\n",
    "    # loop over the number of rows\n",
    "    for y in range(0, numRows):\n",
    "        # extract the scores (probabilities), followed by the geometrical\n",
    "        # data used to derive potential bounding box coordinates that\n",
    "        # surround text\n",
    "        scoresData = scores[0, 0, y]\n",
    "        xData0 = geometry[0, 0, y]\n",
    "        xData1 = geometry[0, 1, y]\n",
    "        xData2 = geometry[0, 2, y]\n",
    "        xData3 = geometry[0, 3, y]\n",
    "        anglesData = geometry[0, 4, y]\n",
    "    \n",
    "        for x in range(0, numCols):\n",
    "    \t\t# if our score does not have sufficient probability, ignore it\n",
    "            # Set minimum confidence as required\n",
    "            if scoresData[x] < 0.5:\n",
    "                continue\n",
    "    \t\t# compute the offset factor as our resulting feature maps will\n",
    "            #  x smaller than the input image\n",
    "            (offsetX, offsetY) = (x * 4.0, y * 4.0)\n",
    "            # extract the rotation angle for the prediction and then\n",
    "            # compute the sin and cosine\n",
    "            angle = anglesData[x]\n",
    "            cos = np.cos(angle)\n",
    "            sin = np.sin(angle)\n",
    "            # use the geometry volume to derive the width and height of\n",
    "            # the bounding box\n",
    "            h = xData0[x] + xData2[x]\n",
    "            w = xData1[x] + xData3[x]\n",
    "            # compute both the starting and ending (x, y)-coordinates for\n",
    "            # the text prediction bounding box\n",
    "            endX = int(offsetX + (cos * xData1[x]) + (sin * xData2[x]))\n",
    "            endY = int(offsetY - (sin * xData1[x]) + (cos * xData2[x]))\n",
    "            startX = int(endX - w)\n",
    "            startY = int(endY - h)\n",
    "            # add the bounding box coordinates and probability score to\n",
    "            # our respective lists\n",
    "            rects.append((startX, startY, endX, endY))\n",
    "            confidences.append(scoresData[x])\n",
    "                        \n",
    "    boxes = non_max_suppression(np.array(rects), probs=confidences)\n",
    "    # loop over the bounding boxes\n",
    "    coordinates = []\n",
    "    for (startX, startY, endX, endY) in boxes:\n",
    "        # scale the bounding box coordinates based on the respective\n",
    "        # ratios\n",
    "        startX = int(startX * rW)\n",
    "        startY = int(startY * rH)\n",
    "        endX = int(endX * rW)\n",
    "        endY = int(endY * rH)\n",
    "        # draw the bounding box on the image\n",
    "        out_image = cv2.rectangle(orig, (startX, startY), (endX, endY), (0, 255, 0), 2)\n",
    "        coordinates.append((startX, startY, endX, endY))\n",
    "    \n",
    "    cv2.imwrite(f\"{output_path}\", out_image)\n",
    "    return coordinates\n",
    "\n",
    "\n",
    "def crop_and_save_image(image_path, coords, output_path):\n",
    "    ''' saves crops of original image according to specified coordinates \n",
    "\n",
    "    param image_path: path to original image\n",
    "    param coordinates: coordinates of bounding boxes in tuple (x_min, y_min, x_max, y_max)\n",
    "    param output_path: path of output file'''\n",
    "    \n",
    "    img = cv2.imread(image_path)\n",
    "    x_min, y_min, x_max, y_max = coords\n",
    "    cropped_img = img[y_min:y_max, x_min:x_max]\n",
    "\n",
    "    # Save\n",
    "    cv2.imwrite(output_path, cropped_img)\n",
    "\n",
    "\n",
    "#############\n",
    "\n",
    "if __name__ == \"__main__\":\n",
    "    \n",
    "    video_frames_dir = \"./testseq_image_frames\" # path to directory containing all frames for video being processed\n",
    "    frame_path = video_frames_dir + \"/frame150.png\" # path to individual frame being processed\n",
    "\n",
    "    base_name = os.path.basename(frame_path) \n",
    "    frame_name, _ = os.path.splitext(base_name) # get just the individual frame name without the extension\n",
    "\n",
    "    os.makedirs(f\"./out_{frame_name}\") # makes output directory for the frame being processed\n",
    "    \n",
    "    image = cv2.imread(frame_path) \n",
    "    coordinates = east_detect(image, f\"./out_{frame_name}/EAST.png\") # get coordinates of bounding boxes and save image to new output dir for that frame\n",
    "\n",
    "    for idx, coords in enumerate(coordinates):\n",
    "        crop_and_save_image(frame_path, coords , f\"./out_{frame_name}/crop_{idx}.jpg\") # save crops of bounding boxes to output dir for that frame\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "798412d1-0c1e-41ee-938d-0125c8976a47",
   "metadata": {},
   "source": [
    "### MMOCR for text recognition"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "be284f68-4ed4-4588-ac40-a7d0a83def17",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "9bd3b312-3d94-479d-9112-bcd264ecd056",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python (HLCV-env)",
   "language": "python",
   "name": "myproject-env"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.0"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
