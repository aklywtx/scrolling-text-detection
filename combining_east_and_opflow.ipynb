{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "from PIL import Image\n",
    "import os, sys\n",
    "import cv2\n",
    "import numpy as np\n",
    "from imutils.object_detection import non_max_suppression\n",
    "import random"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "random.seed(42)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### You can change some hyperparameters here"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "# some hyperparameter\n",
    "frame_interval = 25 \n",
    "east_confidence = 0.5\n",
    "opflow_min_area_threshold = 700\n",
    "opflow_height_increase = 0\n",
    "num_frames_for_overlap_check = 3\n",
    "rolling_text_confidence = 0.2 # if the overlapping area / east box area > 0.2, this is a part of the rolling text"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### You can change the video path here"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "# file path\n",
    "vidpath = 'seq2.mp4'"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Extract frames"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "def files(vidpath):\n",
    "    base_name = os.path.basename(vidpath)\n",
    "    vidname, _ = os.path.splitext(base_name)  # Split the base name to get the name without the extension\n",
    "    \n",
    "    try: # Remove image frames path if it already exists\n",
    "        os.remove(f\"{vidname}_image_frames\")\n",
    "    except OSError:\n",
    "        pass\n",
    "    if not os.path.exists(f\"{vidname}_image_frames\"): # create the directory if it does not already exist\n",
    "        os.makedirs(f\"{vidname}_image_frames\")\n",
    "\n",
    "    src_vid = cv2.VideoCapture(vidpath) # open the video file\n",
    "    return(src_vid)\n",
    "    \n",
    "def process(src_vid, frame_interval):\n",
    "    base_name = os.path.basename(vidpath)\n",
    "    vidname, _ = os.path.splitext(base_name)  # Split the base name to get the name without the extension\n",
    "    saved_frame_nums = []\n",
    "    \n",
    "    index = 0\n",
    "    while src_vid.isOpened():\n",
    "        ret, frame = src_vid.read() \n",
    "        if not ret: # break at the end of the video (ret returns True if frame is succesfully read)\n",
    "            break\n",
    "        name = f'./{vidname}_image_frames/frame{str(index)}.png'\n",
    "\n",
    "        if index % frame_interval == 0: # every {frame_interval} frame will be saved: can adjust this number to capture more or less frames\n",
    "            print('Extracting frame...' + name)\n",
    "            cv2.imwrite(name, frame)\n",
    "            saved_frame_nums.append(index)\n",
    "        index = index + 1\n",
    "        \n",
    "    src_vid.release()\n",
    "    cv2.destroyAllWindows()\n",
    "    \n",
    "    video_frames_dir = f\"./{vidname}_image_frames\"\n",
    "    \n",
    "    return saved_frame_nums, video_frames_dir"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Extracting frame..../seq2_image_frames/frame0.png\n",
      "Extracting frame..../seq2_image_frames/frame25.png\n",
      "Extracting frame..../seq2_image_frames/frame50.png\n",
      "Extracting frame..../seq2_image_frames/frame75.png\n",
      "Extracting frame..../seq2_image_frames/frame100.png\n",
      "Extracting frame..../seq2_image_frames/frame125.png\n",
      "Extracting frame..../seq2_image_frames/frame150.png\n",
      "Extracting frame..../seq2_image_frames/frame175.png\n",
      "Extracting frame..../seq2_image_frames/frame200.png\n",
      "Extracting frame..../seq2_image_frames/frame225.png\n",
      "Extracting frame..../seq2_image_frames/frame250.png\n",
      "Extracting frame..../seq2_image_frames/frame275.png\n",
      "Extracting frame..../seq2_image_frames/frame300.png\n",
      "Extracting frame..../seq2_image_frames/frame325.png\n",
      "Extracting frame..../seq2_image_frames/frame350.png\n",
      "Extracting frame..../seq2_image_frames/frame375.png\n",
      "Extracting frame..../seq2_image_frames/frame400.png\n",
      "Extracting frame..../seq2_image_frames/frame425.png\n",
      "Extracting frame..../seq2_image_frames/frame450.png\n",
      "Extracting frame..../seq2_image_frames/frame475.png\n",
      "Extracting frame..../seq2_image_frames/frame500.png\n",
      "Extracting frame..../seq2_image_frames/frame525.png\n",
      "Extracting frame..../seq2_image_frames/frame550.png\n",
      "Extracting frame..../seq2_image_frames/frame575.png\n",
      "Extracting frame..../seq2_image_frames/frame600.png\n",
      "Extracting frame..../seq2_image_frames/frame625.png\n"
     ]
    }
   ],
   "source": [
    "vid = files(vidpath)\n",
    "saved_frame_nums, video_frames_dir = process(vid, frame_interval)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Optical Flow"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [],
   "source": [
    "import time"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [],
   "source": [
    "def detect_screengrab_boundingboxes(video_path):\n",
    "    ''' Detects significant leftward motion in the input video. \n",
    "        Returns coordinates of bounding boxes of significant leftward motion\n",
    "    '''\n",
    "    vidname = vidname = str(video_path)\n",
    "    cap = cv2.VideoCapture(video_path)\n",
    "    \n",
    "    # Get total number of frames and calculate frame at which point to save\n",
    "    total_frames = int(cap.get(cv2.CAP_PROP_FRAME_COUNT))\n",
    "    save_frame = total_frames // 1.5  # save at ~75%\n",
    "\n",
    "    ret, first_frame = cap.read() # Get the first frame: if not readable ret returns False\n",
    "    if not ret:\n",
    "        print(\"Failed to read the video.\")\n",
    "        return\n",
    "\n",
    "    # Convert first frame to grayscale\n",
    "    prev_gray = cv2.cvtColor(first_frame, cv2.COLOR_BGR2GRAY)\n",
    "\n",
    "    # Initialize a mask for consistent leftward movement\n",
    "    consistent_motion_mask = None\n",
    "    consistency_threshold = 0.8  # Consistent movement threshold (percentage of frames)\n",
    "    num_frames = 0\n",
    "\n",
    "    saved_screengrab = False\n",
    "    screengrab_frame = None\n",
    "    bounding_boxes = []\n",
    "\n",
    "    while cap.isOpened(): # Iterate through each frame of the video\n",
    "        ret, frame = cap.read()\n",
    "        if not ret:\n",
    "            break\n",
    "        \n",
    "        gray = cv2.cvtColor(frame, cv2.COLOR_BGR2GRAY)\n",
    "\n",
    "        # Compute dense optical flow between current and previous frames using Farneback method\n",
    "        # \"flow\" is a 3d array dimensions (img height, img width, 2)\n",
    "        # Channel 1: Horizontal Movement. Channel 2: Vertical Movement\n",
    "        flow = cv2.calcOpticalFlowFarneback(prev_gray, gray, None, 0.5, 5, 15, 3, 5, 1.2, 0)\n",
    "        \n",
    "        # Calculate the mask of leftward movement (negative horizontal movement)\n",
    "        leftward_motion_mask = flow[..., 0] < 0\n",
    "        \n",
    "        # Initialize the consistent motion mask\n",
    "        # with the same shape as leftward_motion_mask to accumulate counts of leftward motion\n",
    "        if consistent_motion_mask is None:\n",
    "            consistent_motion_mask = np.zeros_like(leftward_motion_mask, dtype=np.float32)\n",
    "        \n",
    "        # Update the consistent leftward movement mask where leftward motion is true\n",
    "        consistent_motion_mask[leftward_motion_mask] += 1\n",
    "        num_frames += 1\n",
    "\n",
    "        # Calculate the ratio of consistent leftward movement\n",
    "        consistent_ratio = consistent_motion_mask / num_frames\n",
    "        consistent_leftward_regions = consistent_ratio > consistency_threshold\n",
    "\n",
    "        # Apply morphological closing to fill in the gaps\n",
    "        consistent_leftward_mask = consistent_leftward_regions.astype(np.uint8)\n",
    "        kernel = np.ones((5, 5), np.uint8) \n",
    "        consistent_leftward_mask = cv2.morphologyEx(consistent_leftward_mask, cv2.MORPH_CLOSE, kernel)\n",
    "\n",
    "        # Create an image for highlighting the image where there is consistent leftward motion\n",
    "        consistent_leftward_img = np.zeros_like(frame)\n",
    "        consistent_leftward_img[consistent_leftward_mask == 1] = [0, 255, 0]  # Green\n",
    "\n",
    "        # Use morphological closing (to fill small gaps) and closing (to remove noise) operations to clean leftward-motion mask\n",
    "        kernel = np.ones((5, 5), np.uint8)\n",
    "        cleaned_mask = cv2.morphologyEx(consistent_leftward_mask, cv2.MORPH_CLOSE, kernel)\n",
    "        cleaned_mask = cv2.morphologyEx(cleaned_mask, cv2.MORPH_OPEN, kernel)\n",
    "        \n",
    "        # Save screengrab at save_frame point and extract the bounding boxes\n",
    "        if num_frames == save_frame and not saved_screengrab:\n",
    "            # Convert original frame to grayscale\n",
    "            gray_frame = cv2.cvtColor(gray, cv2.COLOR_GRAY2BGR)\n",
    "\n",
    "            # Overlay green highlight on grayscale frame\n",
    "            overlayed_img = cv2.addWeighted(gray_frame, 0.7, consistent_leftward_img, 0.3, 0)\n",
    "            screengrab_frame = overlayed_img.copy()\n",
    "            saved_screengrab = True\n",
    "\n",
    "            contours, _ = cv2.findContours(cleaned_mask, cv2.RETR_EXTERNAL, cv2.CHAIN_APPROX_SIMPLE)\n",
    "            for cnt in contours:\n",
    "                x, y, w, h = cv2.boundingRect(cnt)\n",
    "                # Only add the bounding box if the width is at least 1.7x the height\n",
    "                if w >= 1.7 * h:\n",
    "                    bounding_boxes.append((x, y, w, h))\n",
    "                    # Draw bounding box on the screengrab\n",
    "                    cv2.rectangle(screengrab_frame, (x, y), (x+w, y+h), (0, 255, 0), 2)  # Green bounding box\n",
    "\n",
    "            cv2.imwrite(f'{vidname}_75percent_80threshold.png', screengrab_frame)\n",
    "    \n",
    "            return bounding_boxes\n",
    "    \n",
    "        prev_gray = gray.copy()\n",
    "    \n",
    "    cap.release()\n",
    "    cv2.destroyAllWindows()\n",
    "\n",
    "    return \"fail\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "seq2.mp4 time taken: 15.93065595626831\n"
     ]
    }
   ],
   "source": [
    "start = time.time()\n",
    "opflow_box_coordinates = detect_screengrab_boundingboxes(vidpath)\n",
    "print(f\"{vidpath} time taken: {time.time()-start}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### EAST for text detection"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [],
   "source": [
    "def east_detect(image, output_path, east_confidence):\n",
    "    ''' uses EAST detection to get x,y coordinates of all bounding boxes of input image \n",
    "    \n",
    "    param images: paths to the image\n",
    "    param output_path: path where to save copy of original image with bounding boxes drawn on top\n",
    "    \n",
    "    returns list of tuples: each tuple contains (x_min, y_min, x_max, y_max) of each detected bounding box\n",
    "\n",
    "    code from: https://medium.com/technovators/scene-text-detection-in-python-with-east-and-craft-cbe03dda35d5\n",
    "    '''\n",
    "    \n",
    "    \n",
    "    layerNames = [\n",
    "    \t\"feature_fusion/Conv_7/Sigmoid\",\n",
    "    \t\"feature_fusion/concat_3\"]\n",
    "\n",
    "    orig = image.copy()\n",
    "    \n",
    "    if len(image.shape) == 2:\n",
    "        image = cv2.cvtColor(image, cv2.COLOR_GRAY2RGB)\n",
    "    \n",
    "    (H, W) = image.shape[:2]\n",
    "    \n",
    "    # set the new width and height and then determine the ratio in change\n",
    "    # for both the width and height: Should be multiple of 32\n",
    "    (newW, newH) = (320, 320)\n",
    "    \n",
    "    rW = W / float(newW)\n",
    "    rH = H / float(newH)\n",
    "    \n",
    "    # resize the image and grab the new image dimensions\n",
    "    image = cv2.resize(image, (newW, newH))\n",
    "    \n",
    "    (H, W) = image.shape[:2]\n",
    "    \n",
    "    net = cv2.dnn.readNet(\"frozen_east_text_detection.pb\")\n",
    "    \n",
    "    blob = cv2.dnn.blobFromImage(image, 1.0, (W, H),\n",
    "    \t(123.68, 116.78, 103.94), swapRB=True, crop=False)\n",
    "    \n",
    "    net.setInput(blob)\n",
    "    \n",
    "    (scores, geometry) = net.forward(layerNames)\n",
    "    \n",
    "    (numRows, numCols) = scores.shape[2:4]\n",
    "    rects = []\n",
    "    confidences = []\n",
    "    # loop over the number of rows\n",
    "    for y in range(0, numRows):\n",
    "        # extract the scores (probabilities), followed by the geometrical\n",
    "        # data used to derive potential bounding box coordinates that\n",
    "        # surround text\n",
    "        scoresData = scores[0, 0, y]\n",
    "        xData0 = geometry[0, 0, y]\n",
    "        xData1 = geometry[0, 1, y]\n",
    "        xData2 = geometry[0, 2, y]\n",
    "        xData3 = geometry[0, 3, y]\n",
    "        anglesData = geometry[0, 4, y]\n",
    "    \n",
    "        for x in range(0, numCols):\n",
    "    \t\t# if our score does not have sufficient probability, ignore it\n",
    "            # Set minimum confidence as required\n",
    "            if scoresData[x] < east_confidence:\n",
    "                continue\n",
    "    \t\t# compute the offset factor as our resulting feature maps will\n",
    "            #  x smaller than the input image\n",
    "            (offsetX, offsetY) = (x * 4.0, y * 4.0)\n",
    "            # extract the rotation angle for the prediction and then\n",
    "            # compute the sin and cosine\n",
    "            angle = anglesData[x]\n",
    "            cos = np.cos(angle)\n",
    "            sin = np.sin(angle)\n",
    "            # use the geometry volume to derive the width and height of\n",
    "            # the bounding box\n",
    "            h = xData0[x] + xData2[x]\n",
    "            w = xData1[x] + xData3[x]\n",
    "            # compute both the starting and ending (x, y)-coordinates for\n",
    "            # the text prediction bounding box\n",
    "            endX = int(offsetX + (cos * xData1[x]) + (sin * xData2[x]))\n",
    "            endY = int(offsetY - (sin * xData1[x]) + (cos * xData2[x]))\n",
    "            startX = int(endX - w)\n",
    "            startY = int(endY - h)\n",
    "            # add the bounding box coordinates and probability score to\n",
    "            # our respective lists\n",
    "            rects.append((startX, startY, endX, endY))\n",
    "            confidences.append(scoresData[x])\n",
    "                        \n",
    "    boxes = non_max_suppression(np.array(rects), probs=confidences)\n",
    "    # loop over the bounding boxes\n",
    "    coordinates = []\n",
    "\n",
    "    if len(boxes) == 0: # if no text/bounding boxes detected\n",
    "        return coordinates\n",
    "        \n",
    "    if len(boxes) >= 1:\n",
    "        for (startX, startY, endX, endY) in boxes:\n",
    "            # scale the bounding box coordinates based on the respective\n",
    "            # ratios\n",
    "            startX = int(startX * rW)\n",
    "            startY = int(startY * rH)\n",
    "            endX = int(endX * rW)\n",
    "            endY = int(endY * rH)\n",
    "            # draw the bounding box on the image\n",
    "            # startX,startY ------\n",
    "            # |                 |\n",
    "            # |                 |\n",
    "            # |                 |\n",
    "            # ----------endX,endY\n",
    "            out_image = cv2.rectangle(orig, (startX, startY), (endX, endY), (0, 255, 0), 2)\n",
    "            # out_image = cv2.rectangle(out_image, (146, 888), (1920, 1002), (0, 255, 0), 2)\n",
    "            coordinates += [[startX, startY, endX, endY]]\n",
    "        \n",
    "        cv2.imwrite(f\"{output_path}\", out_image)\n",
    "        return coordinates\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [],
   "source": [
    "coordinates_list= []\n",
    "\n",
    "for frame_number in saved_frame_nums:\n",
    "    frame_path = video_frames_dir + f\"/frame{frame_number}.png\"  # path to individual frame being processed\n",
    "\n",
    "    \n",
    "    base_name = os.path.basename(frame_path)\n",
    "    frame_name, _ = os.path.splitext(base_name)  # get just the individual frame name without the extension\n",
    "\n",
    "    \n",
    "    if not os.path.isdir(f\"{video_frames_dir}/output\"):\n",
    "        os.makedirs(f\"{video_frames_dir}/output\")  # makes output directory for the frame being processed\n",
    "    \n",
    "    image = cv2.imread(frame_path)\n",
    "\n",
    "    coordinates = east_detect(image, f\"{video_frames_dir}/output/out_{frame_name}_EAST.png\", east_confidence)  # get coordinates of bounding boxes and save image to new output dir for that frame\n",
    "        \n",
    "    coordinates_list += [coordinates]\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Calculate the overlapping area of optic flow rectangles and east rectangles.\n",
    "\n",
    "randomly get {N} frames to do this:\n",
    "* For each opflow rectangle, get the overlapping area sum \n",
    "* The rectangle with the largest overlapping area -> rolling text region \\\n",
    "-> some sanity check for rectangles?"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[(0, 203, 640, 60)]"
      ]
     },
     "execution_count": 12,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "opflow_box_coordinates"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[[],\n",
       " [[496, 208, 610, 249]],\n",
       " [[510, 207, 636, 249], [398, 211, 502, 247]],\n",
       " [[410, 207, 632, 249], [260, 210, 394, 250]],\n",
       " [[172, 210, 282, 249], [538, 210, 638, 253], [292, 205, 538, 247]],\n",
       " [[426, 208, 538, 247],\n",
       "  [70, 210, 172, 249],\n",
       "  [186, 205, 426, 246],\n",
       "  [540, 208, 636, 247]],\n",
       " [[320, 210, 430, 247],\n",
       "  [434, 214, 534, 249],\n",
       "  [80, 205, 322, 247],\n",
       "  [548, 208, 640, 249],\n",
       "  [6, 216, 62, 249]],\n",
       " [[0, 205, 206, 250],\n",
       "  [326, 210, 424, 247],\n",
       "  [212, 208, 320, 249],\n",
       "  [434, 208, 624, 258]],\n",
       " [[536, 213, 640, 247],\n",
       "  [100, 210, 212, 252],\n",
       "  [336, 210, 520, 253],\n",
       "  [216, 214, 322, 250],\n",
       "  [0, 210, 98, 250]],\n",
       " [[224, 205, 404, 255],\n",
       "  [2, 210, 106, 249],\n",
       "  [108, 210, 206, 249],\n",
       "  [428, 208, 574, 247],\n",
       "  [572, 208, 644, 252]],\n",
       " [[322, 211, 456, 249],\n",
       "  [112, 207, 302, 255],\n",
       "  [468, 205, 638, 249],\n",
       "  [6, 211, 94, 247]],\n",
       " [[212, 211, 348, 247], [368, 210, 616, 247], [12, 205, 198, 256]],\n",
       " [[104, 213, 240, 250],\n",
       "  [260, 210, 514, 249],\n",
       "  [564, 205, 644, 247],\n",
       "  [-2, 210, 84, 255]],\n",
       " [[132, 211, 404, 250],\n",
       "  [0, 213, 138, 249],\n",
       "  [446, 199, 550, 250],\n",
       "  [544, 210, 652, 261]],\n",
       " [[36, 211, 296, 250], [440, 205, 630, 258], [342, 202, 440, 250]],\n",
       " [[0, 211, 190, 250],\n",
       "  [242, 201, 332, 249],\n",
       "  [336, 204, 524, 255],\n",
       "  [530, 210, 624, 249]],\n",
       " [[224, 204, 416, 256],\n",
       "  [130, 201, 226, 249],\n",
       "  [424, 205, 524, 247],\n",
       "  [552, 204, 632, 262]],\n",
       " [[24, 205, 114, 252],\n",
       "  [450, 207, 624, 246],\n",
       "  [128, 208, 308, 258],\n",
       "  [308, 208, 406, 249]],\n",
       " [[350, 205, 514, 247],\n",
       "  [14, 198, 208, 255],\n",
       "  [518, 210, 616, 247],\n",
       "  [204, 211, 296, 247]],\n",
       " [[0, 213, 88, 249],\n",
       "  [98, 208, 194, 247],\n",
       "  [226, 202, 408, 250],\n",
       "  [408, 208, 506, 249]],\n",
       " [[130, 205, 296, 249],\n",
       "  [522, 199, 616, 249],\n",
       "  [298, 205, 506, 249],\n",
       "  [-8, 211, 90, 250]],\n",
       " [[18, 207, 194, 250], [194, 210, 294, 246], [416, 198, 516, 250]],\n",
       " [[88, 205, 186, 249], [306, 202, 404, 252], [-6, 213, 80, 250]],\n",
       " [[202, 201, 300, 249], [-14, 207, 154, 249]],\n",
       " [[90, 202, 198, 250]],\n",
       " [[0, 211, 78, 255]]]"
      ]
     },
     "execution_count": 13,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "coordinates_list"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [],
   "source": [
    "def calculate_overlap_area(rect1, rect2):\n",
    "    \"\"\"\n",
    "    Calculate the overlapping area between two rectangles.\n",
    "    \n",
    "    rect1 and rect2 are tuples in the form (startX, startY, endX, endY)\n",
    "    \"\"\"\n",
    "    x1_max = max(rect1[0], rect2[0])\n",
    "    y1_max = max(rect1[1], rect2[1])\n",
    "    x2_min = min(rect1[2], rect2[2])\n",
    "    y2_min = min(rect1[3], rect2[3])\n",
    "    \n",
    "    overlap_width = max(0, x2_min - x1_max)\n",
    "    overlap_height = max(0, y2_min - y1_max)\n",
    "    \n",
    "    if x1_max >= x2_min or y1_max >= y2_min:\n",
    "        return 0\n",
    "    \n",
    "    return overlap_width * overlap_height\n",
    "\n",
    "\n",
    "def sum_overlapping_areas(main_rect, rectangles):\n",
    "    \"\"\"\n",
    "    Calculate the sum of overlapping areas of multiple rectangles with the main rectangle.\n",
    "    \n",
    "    main_rect is a tuple in the form (startX, startY, endX, endY)\n",
    "    rectangles is a list of tuples in the same form\n",
    "    \"\"\"\n",
    "    total_overlap_area = 0\n",
    "    for rect in rectangles:\n",
    "        total_overlap_area += calculate_overlap_area(main_rect, rect)\n",
    "    \n",
    "    return total_overlap_area\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "overlap_areas = []\n",
    "choose_frames = random.sample(list(range(len(saved_frame_nums))), num_frames_for_overlap_check)\n",
    "\n",
    "for opflow_box_coordinate in opflow_box_coordinates:\n",
    "    total_overlap = 0\n",
    "    for frame_index in choose_frames:\n",
    "        total_overlap += sum_overlapping_areas(opflow_box_coordinate, coordinates_list[frame_index])\n",
    "    overlap_areas.append(total_overlap)\n",
    "\n",
    "rolling_box_index = np.argmax(np.array(overlap_areas))\n",
    "rolling_box = opflow_box_coordinates[rolling_box_index]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### After getting the rolling box\n",
    "\n",
    "* For all the east boxes in each frame, calculate their overlapping area with the rolling box\n",
    "* If overlapping area / east box area >= rolling_text_confidence, this is a part of the rolling text\n",
    "* Crop this part and save to the folder in each frame. \\\n",
    "TBD: The cropped text images should follow a left to right sequence"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {},
   "outputs": [],
   "source": [
    "# TODO: The cropped text images should follow a left to right sequence"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {},
   "outputs": [],
   "source": [
    "def crop_and_save_image(image_path, coords, output_path):\n",
    "    ''' saves crops of original image according to specified coordinates \n",
    "\n",
    "    param image_path: path to original image\n",
    "    param coordinates: coordinates of bounding boxes in tuple (x_min, y_min, x_max, y_max)\n",
    "    param output_path: path of output file'''\n",
    "    \n",
    "    img = cv2.imread(image_path)\n",
    "    x_min, y_min, x_max, y_max = coords\n",
    "    cropped_img = img[y_min:y_max, x_min:x_max]\n",
    "\n",
    "    # Save\n",
    "    cv2.imwrite(output_path, cropped_img)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {},
   "outputs": [],
   "source": [
    "for frame_num, rects in zip(saved_frame_nums, coordinates_list):\n",
    "    frame_path = f\"{video_frames_dir}/frame{frame_num}.png\"\n",
    "    output_path = f\"{video_frames_dir}/croppedFrame{frame_num}\"\n",
    "    if os.path.isdir(output_path):\n",
    "        print(\"The cropped folders are there! You can delete them :)\")\n",
    "        continue\n",
    "    else:\n",
    "        os.makedirs(output_path)\n",
    "        \n",
    "    for idx, rect in enumerate(rects):\n",
    "        overlap_area = calculate_overlap_area(rolling_box, rect)\n",
    "        rect_area = (rect[2] - rect[0]) * (rect[3] - rect[1])\n",
    "        if rect_area > 0 and (overlap_area / rect_area) >= rolling_text_confidence:\n",
    "            final_outout_path = f\"{output_path}/{idx}.jpg\"\n",
    "            crop_and_save_image(frame_path, rect, final_outout_path)\n",
    "            print(f\"Saved cropped image to {final_outout_path} for rectangle {rect}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.0"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
