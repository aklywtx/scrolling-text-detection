{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "===================================BUG REPORT===================================\n",
      "Welcome to bitsandbytes. For bug reports, please run\n",
      "\n",
      "python -m bitsandbytes\n",
      "\n",
      " and submit this information together with your error trace to: https://github.com/TimDettmers/bitsandbytes/issues\n",
      "================================================================================\n",
      "bin /Users/aklywtx/opt/anaconda3/lib/python3.9/site-packages/bitsandbytes/libbitsandbytes_cpu.so\n",
      "'NoneType' object has no attribute 'cadam32bit_grad_fp32'\n",
      "CUDA SETUP: Loading binary /Users/aklywtx/opt/anaconda3/lib/python3.9/site-packages/bitsandbytes/libbitsandbytes_cpu.so...\n",
      "dlopen(/Users/aklywtx/opt/anaconda3/lib/python3.9/site-packages/bitsandbytes/libbitsandbytes_cpu.so, 0x0006): tried: '/Users/aklywtx/opt/anaconda3/lib/python3.9/site-packages/bitsandbytes/libbitsandbytes_cpu.so' (not a mach-o file), '/System/Volumes/Preboot/Cryptexes/OS/Users/aklywtx/opt/anaconda3/lib/python3.9/site-packages/bitsandbytes/libbitsandbytes_cpu.so' (no such file), '/Users/aklywtx/opt/anaconda3/lib/python3.9/site-packages/bitsandbytes/libbitsandbytes_cpu.so' (not a mach-o file)\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/Users/aklywtx/opt/anaconda3/lib/python3.9/site-packages/bitsandbytes/cextension.py:34: UserWarning: The installed version of bitsandbytes was compiled without GPU support. 8-bit optimizers, 8-bit multiplication, and GPU quantization are unavailable.\n",
      "  warn(\"The installed version of bitsandbytes was compiled without GPU support. \"\n"
     ]
    }
   ],
   "source": [
    "from PIL import Image\n",
    "import os, sys\n",
    "import cv2\n",
    "from mmocr.apis import MMOCRInferencer\n",
    "import numpy as np\n",
    "from imutils.object_detection import non_max_suppression\n",
    "import random\n",
    "import time\n",
    "import requests\n",
    "import json"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "random.seed(42)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Tune the hyperparameters here"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "# hyperparameters\n",
    "frame_interval = 50 # the interval for extracting frames\n",
    "east_confidence = 0.5 # the confidence threshold for EAST text detection\n",
    "num_frames_for_overlap_check = 3 # the number of frames for checking the overlap of text boxes and optical flow boxes"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Change the video path here"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "# file path\n",
    "vidpath = '../data/train5_crop.mp4'"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Step 1: Extract frames"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "def files(vidpath):\n",
    "    \"\"\"\n",
    "    Extracts frames from the given video path at a specified interval.\n",
    "    \n",
    "    Args:\n",
    "        vidpath (str): Path to the video file.\n",
    "\n",
    "    Returns:\n",
    "        tuple: Source video capture object, saved frame numbers, and directory of video frames.\n",
    "    \"\"\"\n",
    "    base_name = os.path.basename(vidpath)\n",
    "    vidname, _ = os.path.splitext(base_name)  # Split the base name to get the name without the extension\n",
    "    \n",
    "    try: # Remove image frames path if it already exists\n",
    "        os.remove(f\"{vidname}_image_frames\")\n",
    "    except OSError:\n",
    "        pass\n",
    "    if not os.path.exists(f\"{vidname}_image_frames\"): # create the directory if it does not already exist\n",
    "        os.makedirs(f\"{vidname}_image_frames\")\n",
    "\n",
    "    src_vid = cv2.VideoCapture(vidpath) # open the video file\n",
    "    return(src_vid)\n",
    "    \n",
    "def process(src_vid, frame_interval):\n",
    "    \"\"\"\n",
    "    Processes video frames, saving every nth frame as specified by the interval.\n",
    "    \n",
    "    Args:\n",
    "        src_vid (cv2.VideoCapture): Source video capture object.\n",
    "        frame_interval (int): Interval for frame extraction, a hyperparameter\n",
    "\n",
    "    Returns:\n",
    "        tuple: List of saved frame numbers and the directory of video frames.\n",
    "    \"\"\"\n",
    "    base_name = os.path.basename(vidpath)\n",
    "    vidname, _ = os.path.splitext(base_name)  # Split the base name to get the name without the extension\n",
    "    saved_frame_nums = []\n",
    "    \n",
    "    index = 0\n",
    "    while src_vid.isOpened():\n",
    "        ret, frame = src_vid.read() \n",
    "        if not ret: # break at the end of the video (ret returns True if frame is succesfully read)\n",
    "            break\n",
    "        name = f'./{vidname}_image_frames/frame{str(index)}.png'\n",
    "\n",
    "        if index % frame_interval == 0: # every {frame_interval} frame will be saved: can adjust this number to capture more or less frames\n",
    "            print('Extracting frame...' + name)\n",
    "            cv2.imwrite(name, frame)\n",
    "            saved_frame_nums.append(index)\n",
    "        index = index + 1\n",
    "        \n",
    "    src_vid.release()\n",
    "    cv2.destroyAllWindows()\n",
    "    \n",
    "    video_frames_dir = f\"./{vidname}_image_frames\"\n",
    "    \n",
    "    return saved_frame_nums, video_frames_dir"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Extracting frame..../train5_crop_image_frames/frame0.png\n",
      "Extracting frame..../train5_crop_image_frames/frame50.png\n",
      "Extracting frame..../train5_crop_image_frames/frame100.png\n",
      "Extracting frame..../train5_crop_image_frames/frame150.png\n"
     ]
    }
   ],
   "source": [
    "vid = files(vidpath)\n",
    "saved_frame_nums, video_frames_dir = process(vid, frame_interval)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Step 2: Use Optical Flow to get rolling text bouding boxes candidates"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [],
   "source": [
    "def detect_screengrab_boundingboxes(video_path):\n",
    "    ''' \n",
    "    Detects significant leftward motion in the input video and returns bounding boxes of significant motion.\n",
    "    \n",
    "    Args:\n",
    "        video_path (str): Path to the video file.\n",
    "\n",
    "    Returns:\n",
    "        list: Coordinates of bounding boxes of significant leftward motion.\n",
    "    '''\n",
    "    vidname = vidname = str(video_path)\n",
    "    cap = cv2.VideoCapture(video_path)\n",
    "    \n",
    "    # Get total number of frames and calculate frame at which point to save\n",
    "    total_frames = int(cap.get(cv2.CAP_PROP_FRAME_COUNT))\n",
    "    save_frame = total_frames // 1.5  # save at ~75%\n",
    "\n",
    "    ret, first_frame = cap.read() # Get the first frame: if not readable ret returns False\n",
    "    if not ret:\n",
    "        print(\"Failed to read the video.\")\n",
    "        return\n",
    "\n",
    "    # Convert first frame to grayscale\n",
    "    prev_gray = cv2.cvtColor(first_frame, cv2.COLOR_BGR2GRAY)\n",
    "\n",
    "    # Initialize a mask for consistent leftward movement\n",
    "    consistent_motion_mask = None\n",
    "    consistency_threshold = 0.8  # Consistent movement threshold (percentage of frames)\n",
    "    num_frames = 0\n",
    "\n",
    "    saved_screengrab = False\n",
    "    screengrab_frame = None\n",
    "    bounding_boxes = []\n",
    "\n",
    "    while cap.isOpened(): # Iterate through each frame of the video\n",
    "        ret, frame = cap.read()\n",
    "        if not ret:\n",
    "            break\n",
    "        \n",
    "        gray = cv2.cvtColor(frame, cv2.COLOR_BGR2GRAY)\n",
    "\n",
    "        # Compute dense optical flow between current and previous frames using Farneback method\n",
    "        # \"flow\" is a 3d array dimensions (img height, img width, 2)\n",
    "        # Channel 1: Horizontal Movement. Channel 2: Vertical Movement\n",
    "        flow = cv2.calcOpticalFlowFarneback(prev_gray, gray, None, 0.5, 5, 15, 3, 5, 1.2, 0)\n",
    "        \n",
    "        # Calculate the mask of leftward movement (negative horizontal movement)\n",
    "        leftward_motion_mask = flow[..., 0] < 0\n",
    "        \n",
    "        # Initialize the consistent motion mask\n",
    "        # with the same shape as leftward_motion_mask to accumulate counts of leftward motion\n",
    "        if consistent_motion_mask is None:\n",
    "            consistent_motion_mask = np.zeros_like(leftward_motion_mask, dtype=np.float32)\n",
    "        \n",
    "        # Update the consistent leftward movement mask where leftward motion is true\n",
    "        consistent_motion_mask[leftward_motion_mask] += 1\n",
    "        num_frames += 1\n",
    "\n",
    "        # Calculate the ratio of consistent leftward movement\n",
    "        consistent_ratio = consistent_motion_mask / num_frames\n",
    "        consistent_leftward_regions = consistent_ratio > consistency_threshold\n",
    "\n",
    "        # Apply morphological closing to fill in the gaps\n",
    "        consistent_leftward_mask = consistent_leftward_regions.astype(np.uint8)\n",
    "        kernel = np.ones((5, 5), np.uint8) \n",
    "        consistent_leftward_mask = cv2.morphologyEx(consistent_leftward_mask, cv2.MORPH_CLOSE, kernel)\n",
    "\n",
    "        # Create an image for highlighting the image where there is consistent leftward motion\n",
    "        consistent_leftward_img = np.zeros_like(frame)\n",
    "        consistent_leftward_img[consistent_leftward_mask == 1] = [0, 255, 0]  # Green\n",
    "\n",
    "        # Use morphological closing (to fill small gaps) and closing (to remove noise) operations to clean leftward-motion mask\n",
    "        kernel = np.ones((5, 5), np.uint8)\n",
    "        cleaned_mask = cv2.morphologyEx(consistent_leftward_mask, cv2.MORPH_CLOSE, kernel)\n",
    "        cleaned_mask = cv2.morphologyEx(cleaned_mask, cv2.MORPH_OPEN, kernel)\n",
    "        \n",
    "        # Save screengrab at save_frame point and extract the bounding boxes\n",
    "        if num_frames == save_frame and not saved_screengrab:\n",
    "            # Convert original frame to grayscale\n",
    "            gray_frame = cv2.cvtColor(gray, cv2.COLOR_GRAY2BGR)\n",
    "\n",
    "            # Overlay green highlight on grayscale frame\n",
    "            overlayed_img = cv2.addWeighted(gray_frame, 0.7, consistent_leftward_img, 0.3, 0)\n",
    "            screengrab_frame = overlayed_img.copy()\n",
    "            saved_screengrab = True\n",
    "\n",
    "            contours, _ = cv2.findContours(cleaned_mask, cv2.RETR_EXTERNAL, cv2.CHAIN_APPROX_SIMPLE)\n",
    "            for cnt in contours:\n",
    "                x, y, w, h = cv2.boundingRect(cnt)\n",
    "                # Only add the bounding box if the width is at least 1.7x the height\n",
    "                if w >= 1.7 * h:\n",
    "                    bounding_boxes.append((x, y, x+w, y+h))\n",
    "                    # Draw bounding box on the screengrab\n",
    "                    cv2.rectangle(screengrab_frame, (x, y), (x+w, y+h), (0, 255, 0), 2)  # Green bounding box\n",
    "\n",
    "            cv2.imwrite(f'./{vidname}_75percent_80threshold.png', screengrab_frame)\n",
    "    \n",
    "            return bounding_boxes\n",
    "    \n",
    "        prev_gray = gray.copy()\n",
    "    \n",
    "    cap.release()\n",
    "    cv2.destroyAllWindows()\n",
    "\n",
    "    return \"fail\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "../data/train5_crop.mp4 time taken: 55.684545040130615\n"
     ]
    }
   ],
   "source": [
    "start = time.time()\n",
    "opflow_box_coordinates = detect_screengrab_boundingboxes(vidpath)\n",
    "print(f\"{vidpath} time taken: {time.time()-start}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Step 3: Use EAST to choose the best candidate"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [],
   "source": [
    "def east_detect(image, output_path, east_confidence):\n",
    "    '''\n",
    "    uses EAST detection to get x,y coordinates of all bounding boxes of input image \n",
    "    \n",
    "    param images: paths to the image\n",
    "    param output_path: path where to save copy of original image with bounding boxes drawn on top\n",
    "    \n",
    "    returns list of tuples: each tuple contains (x_min, y_min, x_max, y_max) of each detected bounding box\n",
    "\n",
    "    code from: https://medium.com/technovators/scene-text-detection-in-python-with-east-and-craft-cbe03dda35d5\n",
    "    '''\n",
    "    \n",
    "    \n",
    "    layerNames = [\n",
    "    \t\"feature_fusion/Conv_7/Sigmoid\",\n",
    "    \t\"feature_fusion/concat_3\"]\n",
    "\n",
    "    orig = image.copy()\n",
    "    \n",
    "    if len(image.shape) == 2:\n",
    "        image = cv2.cvtColor(image, cv2.COLOR_GRAY2RGB)\n",
    "    \n",
    "    (H, W) = image.shape[:2]\n",
    "    \n",
    "    # set the new width and height and then determine the ratio in change\n",
    "    # for both the width and height: Should be multiple of 32\n",
    "    # (newW, newH) = (320, 320)\n",
    "    (newW, newH) = (1024, 1024)\n",
    "    \n",
    "    rW = W / float(newW)\n",
    "    rH = H / float(newH)\n",
    "    \n",
    "    # resize the image and grab the new image dimensions\n",
    "    image = cv2.resize(image, (newW, newH))\n",
    "    \n",
    "    (H, W) = image.shape[:2]\n",
    "    \n",
    "    net = cv2.dnn.readNet(\"frozen_east_text_detection.pb\")\n",
    "    \n",
    "    blob = cv2.dnn.blobFromImage(image, 1.0, (W, H),\n",
    "    \t(123.68, 116.78, 103.94), swapRB=True, crop=False)\n",
    "    \n",
    "    net.setInput(blob)\n",
    "    \n",
    "    (scores, geometry) = net.forward(layerNames)\n",
    "    \n",
    "    (numRows, numCols) = scores.shape[2:4]\n",
    "    rects = []\n",
    "    confidences = []\n",
    "    # loop over the number of rows\n",
    "    for y in range(0, numRows):\n",
    "        # extract the scores (probabilities), followed by the geometrical\n",
    "        # data used to derive potential bounding box coordinates that\n",
    "        # surround text\n",
    "        scoresData = scores[0, 0, y]\n",
    "        xData0 = geometry[0, 0, y]\n",
    "        xData1 = geometry[0, 1, y]\n",
    "        xData2 = geometry[0, 2, y]\n",
    "        xData3 = geometry[0, 3, y]\n",
    "        anglesData = geometry[0, 4, y]\n",
    "    \n",
    "        for x in range(0, numCols):\n",
    "    \t\t# if our score does not have sufficient probability, ignore it\n",
    "            # Set minimum confidence as required\n",
    "            if scoresData[x] < east_confidence:\n",
    "                continue\n",
    "    \t\t# compute the offset factor as our resulting feature maps will\n",
    "            #  x smaller than the input image\n",
    "            (offsetX, offsetY) = (x * 4.0, y * 4.0)\n",
    "            # extract the rotation angle for the prediction and then\n",
    "            # compute the sin and cosine\n",
    "            angle = anglesData[x]\n",
    "            cos = np.cos(angle)\n",
    "            sin = np.sin(angle)\n",
    "            # use the geometry volume to derive the width and height of\n",
    "            # the bounding box\n",
    "            h = xData0[x] + xData2[x]\n",
    "            w = xData1[x] + xData3[x]\n",
    "            # compute both the starting and ending (x, y)-coordinates for\n",
    "            # the text prediction bounding box\n",
    "            endX = int(offsetX + (cos * xData1[x]) + (sin * xData2[x]))\n",
    "            endY = int(offsetY - (sin * xData1[x]) + (cos * xData2[x]))\n",
    "            startX = int(endX - w)\n",
    "            startY = int(endY - h)\n",
    "            # add the bounding box coordinates and probability score to\n",
    "            # our respective lists\n",
    "            rects.append((startX, startY, endX, endY))\n",
    "            confidences.append(scoresData[x])\n",
    "                        \n",
    "    boxes = non_max_suppression(np.array(rects), probs=confidences)\n",
    "    # loop over the bounding boxes\n",
    "    coordinates = []\n",
    "\n",
    "    if len(boxes) == 0: # if no text/bounding boxes detected\n",
    "        return coordinates\n",
    "        \n",
    "    if len(boxes) >= 1:\n",
    "        for (startX, startY, endX, endY) in boxes:\n",
    "            # scale the bounding box coordinates based on the respective\n",
    "            # ratios\n",
    "            startX = int(startX * rW)\n",
    "            startY = int(startY * rH)\n",
    "            endX = int(endX * rW)\n",
    "            endY = int(endY * rH)\n",
    "            # draw the bounding box on the image\n",
    "            # origin is upper left corner\n",
    "            # startX,startY ------\n",
    "            # |                 |\n",
    "            # |                 |\n",
    "            # |                 |\n",
    "            # ----------endX,endY\n",
    "            startX = max(startX, 0) #TODO: ome starting points of east text bounding boxes are negative numbers. I dont know why\n",
    "            out_image = cv2.rectangle(orig, (startX, startY), (endX, endY), (0, 255, 0), 2)\n",
    "            # out_image = cv2.rectangle(out_image, (146, 888), (1920, 1002), (0, 255, 0), 2)\n",
    "            coordinates += [[startX, startY, endX, endY]]\n",
    "        \n",
    "        cv2.imwrite(f\"{output_path}\", out_image)\n",
    "        return coordinates\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [],
   "source": [
    "def get_text_coordinates_in_all_frames(saved_frame_nums, east_confidence):\n",
    "    \"\"\"\n",
    "    Get the coordinates of the bounding boxes for text detection in all frames.\n",
    "\n",
    "    Args:\n",
    "        saved_frame_nums (list): A list of frame numbers to process.\n",
    "        east_confidence (float): The confidence threshold for text detection.\n",
    "\n",
    "    Returns:\n",
    "        list: A list of coordinates for each frame, where each coordinate is a tuple (x, y, w, h).\n",
    "\n",
    "    \"\"\"\n",
    "    coordinates_list= []\n",
    "\n",
    "    for frame_number in saved_frame_nums:\n",
    "        frame_path = video_frames_dir + f\"/frame{frame_number}.png\"  # path to individual frame being processed\n",
    "\n",
    "        base_name = os.path.basename(frame_path)\n",
    "        frame_name, _ = os.path.splitext(base_name)  # get the individual frame name without the extension\n",
    "        \n",
    "        if not os.path.isdir(f\"{video_frames_dir}/output\"):\n",
    "            os.makedirs(f\"{video_frames_dir}/output\")  # makes output directory for the frame being processed\n",
    "        \n",
    "        image = cv2.imread(frame_path)\n",
    "        coordinates = east_detect(image, f\"{video_frames_dir}/output/out_{frame_name}_EAST.png\", east_confidence)  # get coordinates of bounding boxes and save image to new output dir for that frame  \n",
    "        coordinates_list += [coordinates]\n",
    "        \n",
    "    return coordinates_list"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [],
   "source": [
    "coordinates_list = get_text_coordinates_in_all_frames(saved_frame_nums, east_confidence)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[(942, 798, 1200, 852), (952, 235, 1194, 275)]"
      ]
     },
     "execution_count": 12,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "opflow_box_coordinates"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[[[144, 858, 326, 879],\n",
       "  [277, 351, 399, 376],\n",
       "  [30, 577, 153, 605],\n",
       "  [118, 508, 245, 540],\n",
       "  [28, 862, 75, 883],\n",
       "  [108, 242, 219, 268],\n",
       "  [967, 852, 1020, 875],\n",
       "  [834, 175, 915, 203],\n",
       "  [208, 740, 356, 767],\n",
       "  [403, 340, 688, 374],\n",
       "  [1, 512, 103, 541],\n",
       "  [1096, 851, 1173, 874],\n",
       "  [956, 234, 1076, 256],\n",
       "  [403, 613, 498, 649],\n",
       "  [11, 742, 138, 769],\n",
       "  [31, 465, 166, 490],\n",
       "  [562, 497, 688, 533],\n",
       "  [405, 499, 547, 535],\n",
       "  [161, 288, 288, 311],\n",
       "  [513, 178, 648, 203],\n",
       "  [99, 398, 189, 425],\n",
       "  [393, 565, 558, 603],\n",
       "  [0, 630, 80, 653],\n",
       "  [217, 243, 281, 266],\n",
       "  [99, 625, 217, 650],\n",
       "  [1023, 853, 1080, 876],\n",
       "  [586, 852, 706, 877],\n",
       "  [121, 811, 208, 839],\n",
       "  [376, 683, 600, 719],\n",
       "  [54, 352, 253, 379],\n",
       "  [427, 278, 528, 312],\n",
       "  [768, 854, 915, 876],\n",
       "  [980, 177, 1083, 200],\n",
       "  [1020, 801, 1070, 822],\n",
       "  [166, 465, 232, 489],\n",
       "  [223, 577, 273, 602],\n",
       "  [697, 338, 766, 375],\n",
       "  [472, 854, 583, 879],\n",
       "  [84, 861, 140, 883],\n",
       "  [0, 400, 88, 425],\n",
       "  [414, 230, 616, 266],\n",
       "  [309, 861, 363, 881],\n",
       "  [131, 691, 221, 718],\n",
       "  [0, 244, 95, 271],\n",
       "  [710, 852, 774, 876],\n",
       "  [480, 386, 607, 421],\n",
       "  [1125, 799, 1203, 824],\n",
       "  [609, 679, 701, 716],\n",
       "  [376, 797, 566, 839],\n",
       "  [399, 452, 613, 488],\n",
       "  [101, 293, 157, 314],\n",
       "  [395, 734, 476, 768],\n",
       "  [416, 858, 466, 878],\n",
       "  [245, 84, 451, 141],\n",
       "  [555, 797, 731, 838],\n",
       "  [568, 565, 631, 603],\n",
       "  [523, 612, 603, 648],\n",
       "  [0, 88, 240, 144],\n",
       "  [1061, 801, 1117, 823],\n",
       "  [541, 275, 603, 310],\n",
       "  [498, 730, 558, 766],\n",
       "  [135, 742, 187, 768],\n",
       "  [440, 180, 504, 207],\n",
       "  [365, 859, 406, 880],\n",
       "  [1155, 228, 1207, 249],\n",
       "  [221, 399, 275, 423],\n",
       "  [924, 852, 965, 877],\n",
       "  [146, 577, 198, 603],\n",
       "  [616, 229, 686, 266],\n",
       "  [615, 449, 682, 488],\n",
       "  [1085, 232, 1138, 253],\n",
       "  [948, 803, 988, 822],\n",
       "  [1141, 102, 1192, 133],\n",
       "  [3, 815, 67, 839],\n",
       "  [30, 290, 95, 314],\n",
       "  [279, 577, 328, 601],\n",
       "  [28, 189, 67, 211],\n",
       "  [414, 386, 453, 425]],\n",
       " [[266, 358, 395, 383],\n",
       "  [140, 252, 211, 276],\n",
       "  [22, 586, 142, 610],\n",
       "  [35, 359, 249, 386],\n",
       "  [3, 749, 125, 775],\n",
       "  [146, 866, 305, 888],\n",
       "  [463, 395, 596, 429],\n",
       "  [153, 471, 219, 497],\n",
       "  [211, 251, 277, 276],\n",
       "  [108, 817, 198, 845],\n",
       "  [191, 748, 341, 775],\n",
       "  [129, 700, 208, 726],\n",
       "  [508, 189, 639, 214],\n",
       "  [116, 517, 240, 545],\n",
       "  [915, 185, 1036, 209],\n",
       "  [941, 243, 1038, 265],\n",
       "  [90, 630, 210, 659],\n",
       "  [22, 469, 159, 498],\n",
       "  [386, 463, 600, 494],\n",
       "  [403, 507, 541, 543],\n",
       "  [386, 573, 549, 609],\n",
       "  [408, 241, 607, 275],\n",
       "  [412, 348, 691, 381],\n",
       "  [823, 185, 909, 213],\n",
       "  [150, 295, 283, 318],\n",
       "  [600, 861, 690, 888],\n",
       "  [211, 586, 262, 610],\n",
       "  [0, 410, 67, 432],\n",
       "  [701, 862, 757, 885],\n",
       "  [551, 573, 624, 611],\n",
       "  [414, 285, 526, 320],\n",
       "  [88, 405, 176, 431],\n",
       "  [406, 865, 457, 888],\n",
       "  [468, 863, 566, 889],\n",
       "  [0, 253, 86, 278],\n",
       "  [543, 506, 673, 542],\n",
       "  [373, 691, 586, 726],\n",
       "  [0, 638, 73, 661],\n",
       "  [292, 870, 350, 889],\n",
       "  [388, 621, 493, 660],\n",
       "  [1081, 807, 1190, 836],\n",
       "  [903, 865, 948, 888],\n",
       "  [378, 804, 710, 841],\n",
       "  [598, 688, 688, 724],\n",
       "  [1085, 860, 1166, 885],\n",
       "  [1, 518, 93, 548],\n",
       "  [390, 742, 470, 775],\n",
       "  [958, 863, 1006, 886],\n",
       "  [234, 93, 440, 148],\n",
       "  [965, 813, 1008, 833],\n",
       "  [69, 868, 135, 890],\n",
       "  [684, 348, 751, 383],\n",
       "  [772, 861, 909, 885],\n",
       "  [16, 868, 67, 891],\n",
       "  [1110, 242, 1155, 261],\n",
       "  [1014, 861, 1083, 886],\n",
       "  [0, 97, 221, 152],\n",
       "  [120, 750, 172, 775],\n",
       "  [609, 238, 676, 276],\n",
       "  [510, 620, 586, 657],\n",
       "  [361, 865, 395, 889],\n",
       "  [91, 300, 150, 321],\n",
       "  [534, 283, 600, 320],\n",
       "  [138, 585, 193, 611],\n",
       "  [264, 586, 315, 610],\n",
       "  [433, 187, 495, 216],\n",
       "  [603, 460, 671, 497],\n",
       "  [213, 407, 266, 431],\n",
       "  [30, 298, 80, 321],\n",
       "  [397, 398, 453, 434],\n",
       "  [0, 821, 50, 845],\n",
       "  [487, 740, 545, 775],\n",
       "  [16, 196, 60, 219],\n",
       "  [1046, 243, 1087, 264],\n",
       "  [7, 702, 63, 729],\n",
       "  [808, 563, 879, 631],\n",
       "  [1018, 813, 1061, 836]],\n",
       " [[157, 870, 315, 890],\n",
       "  [515, 193, 652, 217],\n",
       "  [277, 360, 401, 386],\n",
       "  [120, 518, 249, 548],\n",
       "  [151, 360, 258, 386],\n",
       "  [211, 749, 354, 776],\n",
       "  [30, 586, 148, 613],\n",
       "  [123, 818, 208, 846],\n",
       "  [103, 407, 187, 433],\n",
       "  [391, 575, 560, 612],\n",
       "  [478, 397, 605, 431],\n",
       "  [121, 252, 225, 278],\n",
       "  [16, 750, 138, 775],\n",
       "  [135, 702, 215, 727],\n",
       "  [418, 242, 618, 276],\n",
       "  [28, 870, 76, 890],\n",
       "  [5, 522, 103, 549],\n",
       "  [1050, 812, 1134, 838],\n",
       "  [405, 464, 607, 497],\n",
       "  [596, 863, 703, 886],\n",
       "  [50, 361, 163, 389],\n",
       "  [403, 352, 688, 386],\n",
       "  [965, 866, 1016, 888],\n",
       "  [390, 693, 601, 727],\n",
       "  [410, 509, 551, 545],\n",
       "  [311, 870, 361, 889],\n",
       "  [420, 290, 530, 323],\n",
       "  [168, 472, 234, 498],\n",
       "  [781, 865, 913, 886],\n",
       "  [86, 869, 142, 890],\n",
       "  [954, 245, 1010, 265],\n",
       "  [0, 409, 86, 433],\n",
       "  [395, 743, 478, 778],\n",
       "  [380, 808, 588, 850],\n",
       "  [474, 865, 583, 890],\n",
       "  [534, 810, 729, 849],\n",
       "  [397, 622, 508, 661],\n",
       "  [31, 472, 165, 499],\n",
       "  [0, 638, 84, 663],\n",
       "  [556, 508, 686, 545],\n",
       "  [157, 299, 283, 321],\n",
       "  [1095, 863, 1164, 886],\n",
       "  [1066, 245, 1121, 264],\n",
       "  [1130, 814, 1183, 838],\n",
       "  [607, 690, 705, 726],\n",
       "  [924, 186, 1089, 213],\n",
       "  [564, 573, 637, 612],\n",
       "  [838, 187, 920, 214],\n",
       "  [245, 97, 450, 151],\n",
       "  [543, 287, 609, 322],\n",
       "  [221, 586, 271, 611],\n",
       "  [221, 253, 301, 278],\n",
       "  [699, 349, 761, 386],\n",
       "  [418, 868, 466, 889],\n",
       "  [708, 863, 774, 888],\n",
       "  [279, 586, 331, 610],\n",
       "  [103, 632, 219, 659],\n",
       "  [31, 257, 101, 279],\n",
       "  [148, 586, 202, 612],\n",
       "  [0, 100, 241, 152],\n",
       "  [498, 740, 558, 777],\n",
       "  [361, 868, 403, 889],\n",
       "  [1128, 245, 1181, 263],\n",
       "  [523, 620, 603, 659],\n",
       "  [920, 865, 967, 888],\n",
       "  [1021, 865, 1074, 889],\n",
       "  [620, 241, 693, 277],\n",
       "  [990, 814, 1031, 837],\n",
       "  [106, 300, 172, 323],\n",
       "  [446, 189, 506, 218],\n",
       "  [215, 407, 275, 432],\n",
       "  [618, 461, 684, 499],\n",
       "  [133, 751, 185, 776],\n",
       "  [941, 814, 980, 838],\n",
       "  [7, 822, 65, 847],\n",
       "  [37, 302, 95, 325],\n",
       "  [858, 260, 900, 338],\n",
       "  [24, 704, 75, 729],\n",
       "  [1151, 863, 1196, 889],\n",
       "  [826, 488, 877, 555],\n",
       "  [410, 396, 457, 436],\n",
       "  [31, 196, 67, 220],\n",
       "  [1005, 246, 1048, 266]],\n",
       " [[155, 295, 294, 317],\n",
       "  [770, 861, 903, 882],\n",
       "  [965, 860, 1012, 882],\n",
       "  [138, 864, 243, 884],\n",
       "  [28, 469, 166, 495],\n",
       "  [121, 514, 247, 544],\n",
       "  [118, 816, 206, 842],\n",
       "  [828, 182, 916, 210],\n",
       "  [50, 355, 260, 382],\n",
       "  [1095, 860, 1190, 883],\n",
       "  [3, 251, 95, 275],\n",
       "  [588, 859, 706, 884],\n",
       "  [706, 860, 768, 882],\n",
       "  [91, 402, 183, 429],\n",
       "  [1138, 239, 1203, 258],\n",
       "  [241, 864, 307, 886],\n",
       "  [997, 807, 1138, 834],\n",
       "  [129, 699, 213, 724],\n",
       "  [395, 570, 556, 607],\n",
       "  [416, 238, 616, 271],\n",
       "  [468, 394, 609, 427],\n",
       "  [0, 403, 86, 430],\n",
       "  [0, 634, 80, 658],\n",
       "  [99, 297, 163, 318],\n",
       "  [99, 629, 215, 657],\n",
       "  [1021, 860, 1080, 882],\n",
       "  [204, 746, 356, 773],\n",
       "  [275, 356, 397, 380],\n",
       "  [403, 505, 547, 541],\n",
       "  [108, 248, 189, 273],\n",
       "  [420, 345, 690, 380],\n",
       "  [7, 746, 133, 773],\n",
       "  [26, 581, 150, 607],\n",
       "  [693, 344, 759, 380],\n",
       "  [395, 738, 474, 772],\n",
       "  [405, 459, 609, 491],\n",
       "  [84, 863, 133, 888],\n",
       "  [920, 861, 963, 882],\n",
       "  [165, 469, 228, 495],\n",
       "  [22, 864, 75, 888],\n",
       "  [375, 688, 594, 723],\n",
       "  [517, 186, 648, 210],\n",
       "  [556, 504, 686, 541],\n",
       "  [3, 515, 99, 545],\n",
       "  [375, 804, 566, 845],\n",
       "  [421, 284, 526, 317],\n",
       "  [601, 686, 695, 723],\n",
       "  [926, 179, 1050, 206],\n",
       "  [556, 802, 727, 842],\n",
       "  [251, 91, 444, 146],\n",
       "  [397, 619, 504, 656],\n",
       "  [225, 405, 271, 428],\n",
       "  [0, 93, 234, 147],\n",
       "  [560, 570, 633, 609],\n",
       "  [1033, 239, 1083, 258],\n",
       "  [363, 863, 401, 886],\n",
       "  [313, 864, 360, 885],\n",
       "  [219, 584, 275, 607],\n",
       "  [956, 241, 1001, 260],\n",
       "  [414, 862, 463, 885],\n",
       "  [495, 738, 556, 773],\n",
       "  [613, 457, 686, 494],\n",
       "  [500, 861, 581, 885],\n",
       "  [616, 236, 688, 272],\n",
       "  [39, 296, 95, 318],\n",
       "  [440, 183, 502, 211],\n",
       "  [127, 746, 183, 770],\n",
       "  [521, 616, 596, 654],\n",
       "  [223, 246, 285, 273],\n",
       "  [1080, 241, 1130, 260],\n",
       "  [26, 700, 75, 725],\n",
       "  [541, 281, 601, 317],\n",
       "  [142, 583, 196, 608],\n",
       "  [0, 818, 61, 842],\n",
       "  [275, 583, 326, 606],\n",
       "  [1138, 810, 1183, 835],\n",
       "  [935, 808, 990, 831],\n",
       "  [406, 393, 455, 432],\n",
       "  [1141, 108, 1190, 140],\n",
       "  [28, 193, 69, 215],\n",
       "  [858, 247, 894, 323]]]"
      ]
     },
     "execution_count": 13,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "coordinates_list"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [],
   "source": [
    "def calculate_overlap_area(rect1, rect2):\n",
    "    \"\"\"\n",
    "    Calculate the overlapping area between two rectangles.\n",
    "    \n",
    "    Args:\n",
    "        rect1 (tuple): The coordinates of the first rectangle in the form (startX, startY, endX, endY).\n",
    "        rect2 (tuple): The coordinates of the second rectangle in the form (startX, startY, endX, endY).\n",
    "    \n",
    "    Returns:\n",
    "        float: The overlapping area between the two rectangles.\n",
    "    \"\"\"\n",
    "    x1_max = max(rect1[0], rect2[0])\n",
    "    y1_max = max(rect1[1], rect2[1])\n",
    "    x2_min = min(rect1[2], rect2[2])\n",
    "    y2_min = min(rect1[3], rect2[3])\n",
    "    \n",
    "    overlap_width = max(0, x2_min - x1_max)\n",
    "    overlap_height = max(0, y2_min - y1_max)\n",
    "    \n",
    "    if x1_max >= x2_min or y1_max >= y2_min:\n",
    "        return 0\n",
    "    \n",
    "    return overlap_width * overlap_height\n",
    "\n",
    "\n",
    "def calculate_total_overlapping_area(main_rect, rectangles):\n",
    "    \"\"\"\n",
    "    Calculate the total overlapping area of multiple rectangles with the main rectangle.\n",
    "    \n",
    "    Args:\n",
    "        main_rect (tuple): A tuple representing the main rectangle in the form (startX, startY, endX, endY).\n",
    "        rectangles (list): A list of tuples representing the rectangles to calculate the overlapping areas with.\n",
    "                           Each tuple should be in the form (startX, startY, endX, endY).\n",
    "    \n",
    "    Returns:\n",
    "        float: The total overlapping area between the main rectangle and the given rectangles.\n",
    "    \"\"\"\n",
    "    total_overlap_area = 0\n",
    "    for rect in rectangles:\n",
    "        total_overlap_area += calculate_overlap_area(main_rect, rect)\n",
    "    \n",
    "    return total_overlap_area\n",
    "\n",
    "\n",
    "def get_best_opflow_box(opflow_box_coordinates, coordinates_list, num_frames):\n",
    "    \"\"\"\n",
    "    Calculates the rolling box with the maximum overlap area.\n",
    "\n",
    "    Args:\n",
    "        opflow_box_coordinates (list): List of opflow box coordinates.\n",
    "        coordinates_list (list): List of coordinates for each frame.\n",
    "        num_frames (int): Number of frames to consider for overlap check.\n",
    "\n",
    "    Returns:\n",
    "        tuple: The rolling box with the maximum overlap area.\n",
    "\n",
    "    \"\"\"\n",
    "    overlap_areas = []\n",
    "    chosen_frames = random.sample(list(range(len(saved_frame_nums))), num_frames)\n",
    "    \n",
    "    for opflow_box_coordinate in opflow_box_coordinates:\n",
    "        total_overlap = 0\n",
    "        for frame_index in chosen_frames:\n",
    "            total_overlap += calculate_total_overlapping_area(opflow_box_coordinate, coordinates_list[frame_index])\n",
    "        overlap_areas.append(total_overlap)\n",
    "\n",
    "    rolling_box_index = np.argmax(np.array(overlap_areas))\n",
    "    rolling_box = opflow_box_coordinates[rolling_box_index]\n",
    "    \n",
    "    return rolling_box"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [],
   "source": [
    "rolling_box = get_best_opflow_box(opflow_box_coordinates, coordinates_list, num_frames_for_overlap_check)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(942, 798, 1200, 852)"
      ]
     },
     "execution_count": 16,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "rolling_box"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Step 4: Use mmocr to extract texts in rolling boxes"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "After getting the rolling box:\n",
    "\n",
    "We crop the image with opflow boxes\n",
    "\n",
    "but mmocr works badly on them....\n",
    "\n",
    "Adding a wide border around these boxes can give us good results!\n",
    "\n",
    "(Maybe because these boxes are too flat)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {},
   "outputs": [],
   "source": [
    "def crop_and_save_image(image_path, coords, output_path):\n",
    "    '''Saves crops of the original image according to specified coordinates.\n",
    "\n",
    "    Args:\n",
    "        image_path (str): Path to the original image.\n",
    "        coords (tuple): Coordinates of the bounding box in the format (x_min, y_min, x_max, y_max).\n",
    "        output_path (str): Path of the output file.\n",
    "\n",
    "    Returns:\n",
    "        None\n",
    "\n",
    "    Raises:\n",
    "        cv2.error: If there is an error while saving the cropped image.\n",
    "\n",
    "    '''\n",
    "    \n",
    "    img = cv2.imread(image_path)\n",
    "    x_min, y_min, x_max, y_max = coords\n",
    "    cropped_img = img[y_min:y_max, x_min:x_max]\n",
    "    \n",
    "    # Save\n",
    "    try:\n",
    "        cv2.imwrite(output_path, cropped_img)\n",
    "    except cv2.error as e:\n",
    "        print(coords)\n",
    "        print(f\"OpenCV error: {e}\")\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "crop and save bounding boxes"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 28,
   "metadata": {},
   "outputs": [],
   "source": [
    "# adding borders to the boxes:\n",
    "def add_borders(input_img_path, output_img_path):\n",
    "\t\"\"\"\n",
    "\tAdd borders to the input image.\n",
    "\n",
    "\tParameters:\n",
    "\tinput_img_path (str): The file path of the input image.\n",
    "\toutput_img_path (str): The file path to save the output image.\n",
    "\n",
    "\tReturns:\n",
    "\tNone\n",
    "\t\"\"\"\n",
    "\told_im = Image.open(input_img_path)\n",
    "\told_size = old_im.size\n",
    "\t\n",
    "\tnew_size = (old_size[0] + 320, old_size[1] + 512)\n",
    "\tnew_im = Image.new(\"RGB\", new_size, (255, 255, 255))   ## luckily, this is already black!\n",
    "\tbox = tuple((n - o) // 2 for n, o in zip(new_size, old_size))\n",
    "\tnew_im.paste(old_im, box)\n",
    "\t\n",
    "\tnew_im.save(output_img_path)\n",
    "    \n",
    "# Load models into memory\n",
    "def get_ocr_texts(output_path):\n",
    "\t\"\"\"\n",
    "\tExtracts and sorts the OCR texts from the given output path.\n",
    "\n",
    "\tArgs:\n",
    "\t\toutput_path (str): The path to the OCR output file.\n",
    "\n",
    "\tReturns:\n",
    "\t\tlist: A list of sorted OCR texts.\n",
    "\t\"\"\"\n",
    "\tocr = MMOCRInferencer(det='DBNet', rec='SATRN', device='cpu')\n",
    "\n",
    "\t# Perform inference\n",
    "\tresults = ocr(output_path, show=False, print_result=False)\n",
    "\n",
    "\t# Extract the OCR texts as a list of tuples with (word bounding box first coorinate, word)\n",
    "\ttuples_list = [(polygon[0], text) for polygon, text in zip(results[\"predictions\"][0][\"det_polygons\"], results[\"predictions\"][0][\"rec_texts\"])]\n",
    "\n",
    "\t# Sort the list of tuples based on the first number(word bounding box first coorinate) in each tuple\n",
    "\tsorted_tuples = sorted(tuples_list, key=lambda x: x[0])\n",
    "\n",
    "\t# Extract the sorted texts\n",
    "\tsorted_ocr_texts = [text for _, text in sorted_tuples]\n",
    " \n",
    "\treturn sorted_ocr_texts"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 29,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Saved cropped opflow box to ./train5_crop_image_frames/croppedOpflow/frame_num_0.jpg\n",
      "Loads checkpoint by http backend from path: https://download.openmmlab.com/mmocr/textdet/dbnet/dbnet_resnet50-oclip_1200e_icdar2015/dbnet_resnet50-oclip_1200e_icdar2015_20221102_115917-bde8c87a.pth\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/Users/aklywtx/opt/anaconda3/lib/python3.9/site-packages/mmengine/visualization/visualizer.py:196: UserWarning: Failed to add <class 'mmengine.visualization.vis_backend.LocalVisBackend'>, please provide the `save_dir` argument.\n",
      "  warnings.warn(f'Failed to add {vis_backend.__class__}, '\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Loads checkpoint by http backend from path: https://download.openmmlab.com/mmocr/textrecog/satrn/satrn_shallow_5e_st_mj/satrn_shallow_5e_st_mj_20220915_152443-5fd04a4c.pth\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "a58f79b4dde04a248819da50148a71a5",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Output()"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "The model and loaded state dict do not match exactly\n",
      "\n",
      "unexpected key in source state_dict: data_preprocessor.mean, data_preprocessor.std\n",
      "\n"
     ]
    },
    {
     "data": {
      "text/html": [
       "<pre style=\"white-space:pre;overflow-x:auto;line-height:normal;font-family:Menlo,'DejaVu Sans Mono',consolas,'Courier New',monospace\">\n",
       "</pre>\n"
      ],
      "text/plain": [
       "\n"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       "<pre style=\"white-space:pre;overflow-x:auto;line-height:normal;font-family:Menlo,'DejaVu Sans Mono',consolas,'Courier New',monospace\">\n",
       "</pre>\n"
      ],
      "text/plain": [
       "\n"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       "<pre style=\"white-space:pre;overflow-x:auto;line-height:normal;font-family:Menlo,'DejaVu Sans Mono',consolas,'Courier New',monospace\"></pre>\n"
      ],
      "text/plain": []
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       "<pre style=\"white-space:pre;overflow-x:auto;line-height:normal;font-family:Menlo,'DejaVu Sans Mono',consolas,'Courier New',monospace\">\n",
       "</pre>\n"
      ],
      "text/plain": [
       "\n"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Current frame ocr texts are ['Re:', 'Min.', '+++', 'Versp']\n",
      "Saved cropped opflow box to ./train5_crop_image_frames/croppedOpflow/frame_num_50.jpg\n",
      "Loads checkpoint by http backend from path: https://download.openmmlab.com/mmocr/textdet/dbnet/dbnet_resnet50-oclip_1200e_icdar2015/dbnet_resnet50-oclip_1200e_icdar2015_20221102_115917-bde8c87a.pth\n",
      "Loads checkpoint by http backend from path: https://download.openmmlab.com/mmocr/textrecog/satrn/satrn_shallow_5e_st_mj/satrn_shallow_5e_st_mj_20220915_152443-5fd04a4c.pth\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "04f140d7ed7f4bc5b63c2caea9a0b39c",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Output()"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "The model and loaded state dict do not match exactly\n",
      "\n",
      "unexpected key in source state_dict: data_preprocessor.mean, data_preprocessor.std\n",
      "\n"
     ]
    },
    {
     "data": {
      "text/html": [
       "<pre style=\"white-space:pre;overflow-x:auto;line-height:normal;font-family:Menlo,'DejaVu Sans Mono',consolas,'Courier New',monospace\">\n",
       "</pre>\n"
      ],
      "text/plain": [
       "\n"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       "<pre style=\"white-space:pre;overflow-x:auto;line-height:normal;font-family:Menlo,'DejaVu Sans Mono',consolas,'Courier New',monospace\">\n",
       "</pre>\n"
      ],
      "text/plain": [
       "\n"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       "<pre style=\"white-space:pre;overflow-x:auto;line-height:normal;font-family:Menlo,'DejaVu Sans Mono',consolas,'Courier New',monospace\"></pre>\n"
      ],
      "text/plain": []
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       "<pre style=\"white-space:pre;overflow-x:auto;line-height:normal;font-family:Menlo,'DejaVu Sans Mono',consolas,'Courier New',monospace\">\n",
       "</pre>\n"
      ],
      "text/plain": [
       "\n"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Current frame ocr texts are ['Min.', '++', 'Verspatur']\n",
      "Saved cropped opflow box to ./train5_crop_image_frames/croppedOpflow/frame_num_100.jpg\n",
      "Loads checkpoint by http backend from path: https://download.openmmlab.com/mmocr/textdet/dbnet/dbnet_resnet50-oclip_1200e_icdar2015/dbnet_resnet50-oclip_1200e_icdar2015_20221102_115917-bde8c87a.pth\n",
      "Loads checkpoint by http backend from path: https://download.openmmlab.com/mmocr/textrecog/satrn/satrn_shallow_5e_st_mj/satrn_shallow_5e_st_mj_20220915_152443-5fd04a4c.pth\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "a7a626936b704f0584efb214af558c80",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Output()"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "The model and loaded state dict do not match exactly\n",
      "\n",
      "unexpected key in source state_dict: data_preprocessor.mean, data_preprocessor.std\n",
      "\n"
     ]
    },
    {
     "data": {
      "text/html": [
       "<pre style=\"white-space:pre;overflow-x:auto;line-height:normal;font-family:Menlo,'DejaVu Sans Mono',consolas,'Courier New',monospace\">\n",
       "</pre>\n"
      ],
      "text/plain": [
       "\n"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       "<pre style=\"white-space:pre;overflow-x:auto;line-height:normal;font-family:Menlo,'DejaVu Sans Mono',consolas,'Courier New',monospace\">\n",
       "</pre>\n"
      ],
      "text/plain": [
       "\n"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       "<pre style=\"white-space:pre;overflow-x:auto;line-height:normal;font-family:Menlo,'DejaVu Sans Mono',consolas,'Courier New',monospace\"></pre>\n"
      ],
      "text/plain": []
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       "<pre style=\"white-space:pre;overflow-x:auto;line-height:normal;font-family:Menlo,'DejaVu Sans Mono',consolas,'Courier New',monospace\">\n",
       "</pre>\n"
      ],
      "text/plain": [
       "\n"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Current frame ocr texts are ['Jin.', '+++', 'Verspatung']\n",
      "Saved cropped opflow box to ./train5_crop_image_frames/croppedOpflow/frame_num_150.jpg\n",
      "Loads checkpoint by http backend from path: https://download.openmmlab.com/mmocr/textdet/dbnet/dbnet_resnet50-oclip_1200e_icdar2015/dbnet_resnet50-oclip_1200e_icdar2015_20221102_115917-bde8c87a.pth\n",
      "Loads checkpoint by http backend from path: https://download.openmmlab.com/mmocr/textrecog/satrn/satrn_shallow_5e_st_mj/satrn_shallow_5e_st_mj_20220915_152443-5fd04a4c.pth\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "0ff944ac9acd4c21874c070fac78610c",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Output()"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "The model and loaded state dict do not match exactly\n",
      "\n",
      "unexpected key in source state_dict: data_preprocessor.mean, data_preprocessor.std\n",
      "\n"
     ]
    },
    {
     "data": {
      "text/html": [
       "<pre style=\"white-space:pre;overflow-x:auto;line-height:normal;font-family:Menlo,'DejaVu Sans Mono',consolas,'Courier New',monospace\">\n",
       "</pre>\n"
      ],
      "text/plain": [
       "\n"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       "<pre style=\"white-space:pre;overflow-x:auto;line-height:normal;font-family:Menlo,'DejaVu Sans Mono',consolas,'Courier New',monospace\">\n",
       "</pre>\n"
      ],
      "text/plain": [
       "\n"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       "<pre style=\"white-space:pre;overflow-x:auto;line-height:normal;font-family:Menlo,'DejaVu Sans Mono',consolas,'Courier New',monospace\"></pre>\n"
      ],
      "text/plain": []
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       "<pre style=\"white-space:pre;overflow-x:auto;line-height:normal;font-family:Menlo,'DejaVu Sans Mono',consolas,'Courier New',monospace\">\n",
       "</pre>\n"
      ],
      "text/plain": [
       "\n"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Current frame ocr texts are ['+++', '-', 'Versplatung', 'the']\n"
     ]
    }
   ],
   "source": [
    "\"\"\"\n",
    "1. Creates a directory to store cropped opflow images.\n",
    "2. Iterates over a list of frame numbers and performs the following operations for each frame:\n",
    "   a. Retrieves the path of the frame image.\n",
    "   b. Crops and saves the opflow image using a rolling box.\n",
    "   c. Prints the path of the saved cropped opflow image.\n",
    "   d. Adds borders to the cropped opflow image.\n",
    "   e. Extracts OCR texts from the opflow image with borders.\n",
    "   f. Prints the OCR texts for the current frame.\n",
    "   g. Appends the OCR texts to a list.\n",
    "\n",
    "Note: The functions `crop_and_save_image`, `add_borders`, and `get_ocr_texts` are defined above\n",
    "\"\"\"\n",
    "\n",
    "ocr_texts = []\n",
    "\n",
    "output_path = os.path.join(video_frames_dir, \"croppedOpflow\")\n",
    "if os.path.isdir(output_path):\n",
    "    print(\"The cropped folders exists! Please delete them :)\")\n",
    "else:\n",
    "    os.makedirs(output_path)\n",
    "\n",
    "for frame_num in saved_frame_nums:\n",
    "    frame_path = os.path.join(video_frames_dir, f\"frame{frame_num}.png\")\n",
    "        \n",
    "    opflow_crop_path = os.path.join(output_path, f\"frame_num_{frame_num}.jpg\")\n",
    "    crop_and_save_image(frame_path, rolling_box, opflow_crop_path)\n",
    "    \n",
    "    print(f\"Saved cropped opflow box to {opflow_crop_path}\")\n",
    "    \n",
    "    opflow_crop_with_border_path = os.path.join(output_path, f\"frame_num_{frame_num}_border.jpg\")\n",
    "    add_borders(opflow_crop_path, opflow_crop_with_border_path)\n",
    "    cur_ocr_texts = get_ocr_texts(opflow_crop_with_border_path)\n",
    "    print(f\"Current frame ocr texts are {cur_ocr_texts}\")\n",
    "    ocr_texts.append(cur_ocr_texts)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 30,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "['Re:', 'Min.', '+++', 'Versp']\n",
      "['Min.', '++', 'Verspatur']\n",
      "['Jin.', '+++', 'Verspatung']\n",
      "['+++', '-', 'Versplatung', 'the']\n"
     ]
    }
   ],
   "source": [
    "for text in ocr_texts:\n",
    "\tprint(text)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Step 5: Use llama to concatenate the texts"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 31,
   "metadata": {},
   "outputs": [],
   "source": [
    "def llama3(prompt):\n",
    "    \"\"\"\n",
    "    Sends a prompt to the Llama3.1 model and returns the response.\n",
    "\n",
    "    Args:\n",
    "        prompt (str): The prompt to send to the Llama3.1 model.\n",
    "\n",
    "    Returns:\n",
    "        str: The response from the Llama3.1 model.\n",
    "    \"\"\"\n",
    "    url = \"http://localhost:11434/api/chat\"\n",
    "    data = {\n",
    "        \"model\": \"llama3.1\",\n",
    "        \"messages\": [\n",
    "            {\n",
    "              \"role\": \"user\",\n",
    "              \"content\": prompt\n",
    "            }\n",
    "        ],\n",
    "        \"stream\": False\n",
    "    }\n",
    "    \n",
    "    headers = {\n",
    "        'Content-Type': 'application/json'\n",
    "    }\n",
    "    \n",
    "    response = requests.post(url, headers=headers, json=data)\n",
    "    return(response.json()['message']['content'])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 35,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Der Minister verspätet sich immer wieder.\n"
     ]
    }
   ],
   "source": [
    "prompt = f\"Can you please take these word lists extracted from each frame of a rolling text video, concatenate them based on overlapping parts, and provide me with the resulting sentence? Please keep the original language, don't translate. Don't explain; provide only the sentence. {ocr_texts}\"\n",
    "response = llama3(prompt)\n",
    "print(response)"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.13"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
